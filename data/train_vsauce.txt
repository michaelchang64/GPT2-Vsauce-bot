<|startoftext|>

Hey, Vsauce. Michael here. Let's take a moment to recognize the heroes who count. Canadian Mike Smith holds the world record for the largest number counted to in one breath - 125. But the world record for the largest number ever counted to belongs to Jeremy Harper from Birmingham, Alabama. In order to set the record, Harper never left his apartment. He got regular sleep, but from the moment he woke up in the morning until the moment he went to bed at night, Harper did nothing but count. He streamed the entire process over the Internet and raised money for charity while doing it, but after three months of counting all day, every day, he finally reached the world record - 1 million. Now, a million might not sound like a lot, but think of this way. One thousand seconds is about 17 minutes, but a million seconds is more than 11 days. And a billion seconds, well, that's more than 31 years. There's no full video online of Harper counting all the way to a million, but you can watch John Harchick count all the way to 100,000, if you have 74 hours to spare. John also has some other channels. One involves more than 300 videos of himself eating carrots. Another, more than 3,000 videos of himself drinking water. Many of John's videos literally have no views.



They are as lonely as a video on YouTube can get. A great way to find such videos is a website made by Jon van der Kruisen. This website auto-plays videos on YouTube that no one has yet watched. John and Jeremy, as well as Mike, the one breath counter counted like this. 1, 2, 3, 4, 5, 6, 7 and so on. But that's not the only way to count. And it doesn't seem to be the one we're born with.



Additive counting is the one we're all familiar with, where each next step is just one added to the last. But what if we multiply it by a number instead? Well, that kind of counting is logarithmic, from "arithmos" meaning number and "logos" meaning ratio, proportion. On this scale, similar distances are similar proportions. One is a third of three and three is a third of nine. Four is a third of 12 and so on. Our brains perceive the world around us on a logarithmic scale. It's believed that almost all of our senses are multiplicative, not additive.



For example, how loud we perceive a sound to be. Two boomboxes playing at the same volume don't sound twice as loud as one. In order to make a sound that is perceived as being about twice as loud as one boombox, you actually need ten times as many, so 10. And to double that loudness, you would need a hundred. And to double that loudness, you would need a thousand. Having an intuitive sense of logarithmic scales built into your brain is probably an advantage when it comes to natural selection and survival, because often proportion matters more than absolute value. For example, "is there one lion hiding over there in the shadows or two?" is a very different question than "are there ninety six lions about to attack us or ninety seven?" Sure, in both cases I'm just talking about one extra lion, but adding one lion to one lion, doubles the threat. Adding one lion to 96, well, that's basically nothing.



Logarithmic thinking and feeling may explain why life seems to speed-up as we get older. It seems like I was a child for ever. And in college, in my early 20's, just whizzed by. And logarithmically, that makes sense, because each new year that I live is the smaller fraction of all the other years I've already lived. When you turn 2 years old, the last year of your life is half your life. But when you turn 81, that last year that you've lived, well, that's just a tiny part of the other 80 that you know. Logarithmic thinking isn't always helpful, especially in scenarios where proportion doesn't logically matter but we, nonetheless, act like it does. One of my favorite examples is the psychophysics of price paradox. This is something almost all of us do.



Researchers found consistently that people are willing to put a lot of effort into saving 5 dollars of a 10 dollar purchase, but they won't put much effort into saving 5 five dollars of a 2,000 dollar purchase. It's 5 dollars saved either way, but our natural obsession with proportion leads us astray. Take a look at these pictures. Can you tell how many objects are in each of them? You probably can. It's like really easy. You can tell if there are zero, one, two, three or four objects in a photo without even needing to count. How are you doing that? Is it some sort of sixth sense? No.



Psychologists call it "subitizing." We can, intuitively, at a glance, determine whether there are about four or fewer objects in a photo. This has been part of human culture for a very long time and it may be the reason so many tally systems from all over the world all through history wind up having to do something different when counting the number five. When estimating or comparing amounts above 4, the brain uses what's known as an approximate number system. It's a psychological ability we have. It's about 15 percent accurate. It two amounts are at least 15 percent different, we can tell. So, for example, 100 objects and 115 or a thousand and 1,150 or 1,200.



If you wanna test the accuracy of your approximate number system Panamath has a pretty good test. We often take linear additive counting for granted, but it's not granted to us. We aren't born with it. We are, however, born with the ability to subitize and use an approximate number system. Children younger than the age of three can tell, without counting, that this line of 4 coins contains fewer coins than this line of 6, even if you spread the 4 coins out into a line that is physically bigger, longer than this line of 6. However, mysteriously, around the age of 3.5, children lose this ability and begin saying that this line of 6 coins contains fewer coins than this long line of just 4 coins, possibly because around this age the physical world of objects, physical sizes, is more salient in their minds. But then, when they begin to learn linear counting, they reverse back and begin again correctly saying that this line of 6 contains more coins than this line of 4, around the age of 4.



The smallest physical thing science could ever hope to observe is the Planck length. In order to look at anything smaller, you'd need to have so much energy concentrated in such a small area a black hole would form and you would lose whatever you were looking at. Okay, with that in mind, here's a question. What number is halfway in-between 1 and 9. 5 seems like the obvious answer. There are four numbers on either side of 5, it's halfway between, right? Well, if you ask this question of a young child or a member of a culture that doesn't teach a linear additive number line, their answer will be 3. You see, they are exhibiting the human mind's natural logarithmic tendency, because 3 in that sense makes sense. Three is three times larger than 1, and 9 is three times larger than 3. Three is in the middle, proportionately.



But what if we took that logarithmic number line and change the one to be the smallest thing we can observe, the Planck length, and the nine to be the largest thing we can observe, the observable universe. What would go in the middle? Well, as it turns out, we would. The number of Planck lengths you could stretch across a brain cell is equal to the number of your brain cells it would take to stretch all the way across the observable universe. sold So, welcome to the middle. And as always, thanks for watching. Hello again. The YouTube channel Field Day recently gave me an opportunity to explore Whittier, Alaska, one of the strangest places humans call home. To see why and to see me investigate, talk to the locals, click the link in this video's description or on the annotation here on this video. It was really fun, so give it a little lookie look.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. Where are your fingers? Seriously. It's a pretty easy question. You should be able to answer it. But how do you know? How does anyone know anything?



You might say, well, I know where my fingers are. I'm looking right at them. Or, I can touch them, I can feel them, they're right here and that's good. Your senses are a great way to learn things. In fact, we have way more than the usual five senses we talk about. For instance, your kinesthetic sense, proprioception. This is what the police evaluate during a field sobriety test. It allows you to tell where your fingers and arms and head and legs in your body is all in relation to each other without having to look or touch other things. We have way more than five senses, we have at least twice as many and then some. But they're not perfect.



There are optical illusions, audio illusions, temperature sensation illusions, even tactile illusions. Can you turn your tongue upside down? If so, perfect. Try this. Run your finger along the outer edge of the tip of your upside down tongue. Your tongue will be able to feel your finger, but in the wrong place. Our brains never needed to develop an understanding of upside down tongue touch. So, when you touch the right side of your tongue when it's flipped over to your left side you perceive a sensation on the opposite side, where your tongue usually is but isn't when it's upside down. It's pretty freaky and cool and a little humbling, because it shows the limits of the accuracy of our senses, the only tools we have to get what's out there in here.



The philosophy of knowledge, the study of knowing, is called epistemology. Plato famously said that the things we know are things that are true, that we believe and that we have justification for believing. those justifications might be irrational or they might be rational, they might be based on proof, but don't get too confident because proven is not a synonym for true. Luckily, there are things that we can know without needing proof, without needing to even leave the house, things that we can know as true by reason alone. These are things that we know a priori. An example would be the statement "all bachelors are unmarried." I don't have to go survey every bachelor on earth to know that that is true. All bachelors are unmarried because that's how we define the word bachelor. Of course, you have to know what the words bachelor and unmarried mean in the first place. Oh, you do? Okay. Perfect. That's great. But how do you know?



This time I mean functionally, how do you know? Where is knowledge biologically in the brain? What are memories made out of? We are a long way from being able to answer that question completely but research has shown that memories don't exist in the brain in single locations. Instead, what we call a memory is likely made up of many different complex relationships all over the brain between lots of brain cells, neurons. A major cellular mechanism thought to underlie the formation of memories is long-term potentiation or LTP. When one neuron stimulates another neuron repeatedly that signal can be enhanced overtime LTP, wiring them more strongly together and that connection can last a long time, even an entire lifetime. A collection of different brain cells, neurons that fire together in a particular order over and over again frequently and repeatedly can achieve long-term potentiation, becoming more sensitive to each other and more ready to fire in the exact same way later on in the future. They're a physical thing in your brain, firing together more easily because you strengthen that pattern of firing. You memorized. This branching forest of firing friends looks messy, but look closer. It could be the memory of your first kiss. A living souvenir of the event. If I were to go into your brain and cut out those cells, could I make you forget your first kiss or could I make you forget where your fingers are? Only if I cut out a lot of your brain. Because memories aren't just stored in one relationship, they're stored all over the brain. The events leading up to your first kiss are stored in one network, the way it felt to the way it smelled in different networks, all added up together making what you call the memory of your first kiss.



How many memories can you fit inside your head? What is the storage capacity of the human brain? The best we can do is a rough estimate, but given the number of neurons in the brain involved with memory and the number of different connections a single neuron can make Paul Reber at Northwestern University estimated that we can store the digital equivalent of about 2.5 petabytes of information. That's the equivalent of recording a TV channel continuously for 300 years. That's a lot of information. That is a lot of information about skills you can do and facts and people you've met, things in the real world. The world is real, right? How do you know?



It's a difficult question, but it's not rocket science. Instead, it is asking whether or not rocket scientists even exist in the first place. The theory that the Sun moved around the earth worked great. It predicted that the Sun would rise every morning and it did. It wasn't until later that we realized what we thought was true might not be. So, do we or will we ever know true reality or are we stuck in a world where the best we can do is be approximately true? Discovering more and more useful theories every day but never actually reaching true objective actual reality. Can science or reason ever prove convincingly that your friends and YouTube videos and your fingers actually exist beyond your mind? That you don't just live in the matrix?



No. Your mind is all that you have, even if you use instruments, like a telescope or particle accelerators. The final stop for all of that information is ultimately you. You are alone in your own brain, which technically makes it impossible to prove that anything else exists. It's called the egocentric predicament. Everything you know about the world out there depends on and is created inside your brain. This mattered so much to Charles Sanders Peirce that he drew a line between reality, the way the universe truly is, and what he called the phaneron, the world as filtered through our senses and bodies, the only information we can get. If you want to speak with certainty you live in, that is you react to and remember and experience your phaneron, not reality. The belief that only you exist and everything else, food, the universe, your friends are all figments of your mind is called solipsism. There is no way to convince a solipsist that the outside world is real. And there is no way to convince someone who doubts that the universe wasn't created just three seconds ago along with all of our memories. It's a frightening realization that we don't always know how to deal with. There's even The Matrix defense.



In 2002 Tonda Lynn Ansley shot and killed her landlady. She argued that she believed she was in the matrix, that her crimes weren't real. By using the matrix defense, she was found not guilty by reason of insanity, because the opposite view is just way healthier and common. It's called realism. Realism is the belief that the outside world exists independently of your own phaneron. Rocks and stars and Thora Birch would continue to exist even if you weren't around to experience them. But you cannot know realism is true. All you can do is believe.



Martin Gardner, a great source for math magic tricks, explained that he is not a solipsist because realism is just way more convenient and healthy and it works. As to whether it bothered him that he could never know realism was true, he wrote, "If you ask me to tell you anything about the nature of what lies beyond the phaneron, my answer is how should I know? I'm not dismayed by ultimate mysteries, I can no more grasp what is behind such questions as my cat can understand what is behind the clatter I make while I type this paragraph." Humble stuff. What strikes me is the cat.



Cats do not understand keyboards, but they know the keyboards are a fun place to be. It's a great way to get the attention of a human, they're warm and exciting, surrounded by noises and flashing lights plus cats love to get their scent on whatever they can, a mark of their existence. We aren't that much different, except instead of keyboards we have the mysteries of the universe. We will never be able to understand all of them.



We won't be able to ever answer every single question, but walking around in those questions, exploring them, is fun. It feels good. And as always, thanks for watching. Do you want more unanswered questions? Well, you're in luck. Today, nine other amazing channels on YouTube have made videos about questions we still haven't fully answered. Alltime10s has organized them and to watch them all click the annotation at the end of this video or the link at the top of the description. Enjoy.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. In 2003, researchers did the measurements and found that Kansas is in fact literally flatter than a pancake. Of course, the Earth is not flat, the Earth is round. Otherwise travellers would be falling off the edge all the time. Right? Wrong. 



If the Earth was not a ball shaped, but was instead a flat disk, like this plate, well with the weight, density and thickness, living in the middle could feel pretty normal. But as you move toward the edge, gravity on a disk Earth would slightly skew, pushing at a greater and greater angle back toward the centre. My friend Nick from 'yeti dynamics' put together this great simulation. The person and buildings obviously aren't to scale but check out how such increasingly diagonal gravity would work. Although this is a flat disk, it would feel to a runner headed toward the edge, like they were fighting to climb up a steeper and steeper hill. The building foundations behind the runner reflect how you would have to build structures, closer and closer to the edge, so that people living in them always felt like down was at right angles to the floor - the way we feel it on our big, round Earth. As you approach the edge, things would get scary. Remember, this is a flat Earth, but it would feel like a sheer drop off. What's really cool is that contrary to the "don't fall off the edge" fear, on a flat world because of gravity, the scary risk would actually be falling away from the edge and rolling all the way back to the centre. Once you stepped over the edge, instead of falling off into space, you'd be able to relax. It would be a nice level place. 



This model, of course, neglects the fact that such a planet shape would be impossible. Anything as massive as the Earth, shaped like a flat disc, would, under its own gravity, naturally collapse back into a ball. This is why in outer space everything more than few hundred miles in diameter is round. Or so we've been told. What if gravity isn't real? What if the Earth is, in fact, flat and science has been wrong all along? 



It's a misconception that Christopher Columbus discovered that the Earth is round. Virtually every scholar and major religion in the West accepted Earth's rotundity, since at least the time of the Ancient Greeks, who, for instance, had noticed that boats disappear bottom first when sailing away. And, as you walk north and south, stars pop in and out of the view. The misconception that only a few hundred years ago lots and lots of people believed the Earth was flat likely began in the modern era, as a sort of insult. Well, your people recently thought the Earth was flat, so why should we believe you now? The smear was repeated and published so often it became accepted as historical fact. "Flat-Earther" became synonymous with "Anti-science". It might seem flat over short distances, but over longer ones, well the Earth is pretty darn curvy. The Verrazano–Narrows Bridge, connecting Staten Island and Brooklyn, had to be designed with Earth's roundness in mind. Its 2 towers, separated by 1300 metres, and perfectly vertical, are nonetheless 41 milimetres further apart at the top than at the bottom because of Earth's curvature. In the 3rd century BC, Eratosthenes measured the differences between shadows cast by poles in Syene and Alexandria to calculate, more than 2000 years before rockets and space travel, the circumference of the entire globular Earth, with, for the time, impressive accuracy. Word got around that the Earth was a round shape after that. 



But in 1906, Wilbur Glenn Voliva became head of a slightly bizarre religious sect that pretty much ran the city of Zion, Illinois. Voliva believed that the Earth was actually flat and he enforced flat Earth's teachings in schools in Zion. He also enforced that belief on really anyone who entered the city. Voliva believed not only that the Earth was flat, but that the sun was only few thousand miles away from Earth. Not 93 million. He also believed that the sun was only 32 miles across, not 860 000. He sounds crazy, or does he? You see, the same phenomenon Eratosthenes measured could be explained by a flat Earth, if the sun were only few thousand miles away and 32 miles across - the math would work out the same. 



Today, with the power of the Internet, modern day flat Earthers have picked up where Voliva left off. They have quite good explanations for any evidence you throw at them that the Earth is round. Circumnavigation is really just a flat circle path. The round shadow Earth casts on the Moon during a lunar eclipse could also be made by a flat disc. Time zones are caused by spotlight sun, and remember how gravity would be totally different on a disc-shaped planet? Well, they argue that gravity, as we know it, simply doesn't exist. The flat disc of Earth is merely accelerating up at 9.8 metres per second. 



As for all of the photos and video evidence we now have that the Earth is round, thanks to space exploration, well all of that material is completely fabricated. A hoax, perpetrated by Big Globe. Space agencies, airlines, globe manufacturers. They are reaping the rewards of our ignorant belief that the Earth is actually round. They know, of course, that it's flat. And they're hiding that truth from us. Is it merely a coincidence that the logo used by the Flat Earth Society is a projection of Earth, centred on the North Pole, and also happens to be the projection used by the United Nations? Are these people for real? Probably not most of them. 



But this is the crocks of Poe's Law. An adage that states that at their extremes, parody of extremism and sincere extremism are difficult to distinguish. Although clever, flat Earth theories are predominantly ad hoc explanations - excuses made up on the spot that only address one issue and don't fit all the evidence. Science, of course, rejects a theory if a better one fits more of our observations, but why the egoistical obsession with OUR observations? A cosmic ray particle could use the very same scientific method we use and conclude that the Earth was, in fact, flat. 



You see, at speeds near the speeds of light, time slows down and lengths contract. One way we know this is that unstable muons, created in the upper atmosphere by the collision of cosmic rays with the atmosphere, should mostly decay before reaching Earth's surface. But yet, we detect a lot of them down here, because they're crazy fast speed literally means that, from our perspective, their physics runs according to a slower clock; and to them, the distance they have to cover to the surface during their short lives is, from their perspective, much much shorter than it appears to us. If you're a cosmic ray proton travelling at 99.9999999999991% the speed of light, Earth would appear to be only 17 metres thick in the direction you travel. So Earth is flat to them, but round to us. It is ball shaped to some observers and flat to others. There doesn't appear to be a single most correct-est, in all circumstances, answer. 



Susan Haack compares knowledge to a crossword puzzle. New answers interweave with old ones, they all reinforce one another. The clues are the questions we ask, and the way the answers fall into a predetermined grid, well, that's our confidence that we're on the right track. But that doesn't mean one day there will be a finished puzzle - a complete answer. Recall The New York Times famous 1996 crossword puzzle that came out the day before the US election between Bill Clinton and Bob Dole. The clue for 39 across was pretty crazy. You seemed to need to be able to tell the future to answer it correctly. It simply said, "Lead story in tomorrow's newspaper (blank) elected". Well that blank could be Clinton or Bob Dole and who's to say which one until tomorrow? There's no way to know. But, as it turned out, the answer was Clinton, or Bob Dole. No matter which you wrote in, all the other clues fit. For instance, a "black Halloween animal" could either be a cat or a bat. Our knowledge about the outside world might be the same. A puzzle with no answer key, just the reassurance that the answers we think we know fit together, so they're probably correct. Though there's always the possibility that the answer to one clue, or all of them, will fundamentally not have a single definite satisfying answer. The puzzle may be playable forever. 



I like what Richard Feynman says about this. "Some people say 'How can you live without knowing?' I do not know what they mean. I always live without knowing - that is easy. How you get to know is what I want to know." You know? 



And as always, thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. This appears blue. This appears yellow, and this appears green. Those of us with normal color vision can probably agree. But that doesn’t change the fact that color is an illusion. Color as we know it does not exist in the outside world, beyond us, like gravity or protons do. Instead, color is created inside our heads. Our brains convert a certain amount of the electromagnetic spectrum into color. I can measure the wavelength of radiation, but I can’t recreate the experience of color inside your mind. So, how do I know that when you and me look at a strawberry, and this perception occurs [a red strawberry],which I call “red”, that in your brain this perception doesn’t occur [a green strawberry], which you have, of course, also learned to call “red”? We both call it “red”, we communicate effectively, and we walk away, never knowing just how different each of our internal experiences really were.



Of course, we already know that not everybody sees color in exactly the same way. One example would be color blindness. But we can diagnose and discuss these differences because people with the conditions fail to see things that most of us can. Conceivably, though, there could be ways of seeing that we use that cause colors to look differently in different people’s minds without altering their performances on any tests we could come up with. Of course, if that were the case, wouldn’t some people think other colors looked better than others? Or that some colors were more complimentary of others? Well, yeah, but doesn’t that already happen? This matters, because it shows that fundamentally, in terms of our perceptions, we are all alone in our minds.



Let’s say I met an alien from a faraway solar system who, lucky enough, could speak English, but had never, and could never feel pain. I could explain to the alien that pain is sent through the A-delta and C-fibers to the spinal cord, the alien could learn every single cell and pathway and process and chemical involved in the feeling of pain, the alien could pass a biology exam about pain, and believe that pain, to us, is generally a bad thing. But no matter how much he learned, the alien would never actually feel pain. Philosophers call these ineffable raw feelings “qualia”, and our inability to connect words to these raw feelings, our inability to explain and share our own internal qualia is known as the Explanatory Gap. 



This gap is confronted when describing color to someone who has been blind their entire life. Tommy Edison has never been able to see. He has a YouTube channel where he describes what being blind is like. It’s an amazing channel. In one video, he talks about colors, and how strange and foreign of a concept it seems to him. Sighted people try to explain, for instance, that red is “hot”, and blue is “cold”, but to someone who has never seen a single color, that just seems weird. And, as he explains, it has never caused him to finally “see” a color.



Some philosophers, like Daniel Dennett, argue that qualia may be private and ineffable simply because of a failure in our own language, not because they are necessarily always going to be impossible to share. There may be an alien race that communicates in a language that causes colors to “appear” in your brain without your retina having to be involved at all. Or, without you ever having to see the color yourself. Perhaps even in English, given millions and billions of words used in just the right way, it may be possible to adequately describe a color such that a blind person could see it for the first time. Or, you could figure out that, once and for all, yes, or no in fact, you and your friend do not see the same red. But for now, it remains the case that we have no way of knowing if my red is the same as your red. Maybe one day our language will allow us to share and find out, or maybe it never will. I know it’s frustrating to not have an answer, but the mere fact that you guys can ask me about my internal experiences, and the mere fact that I can ask my friends, and we can all wonder at the mere concept of qualia is quite incredible, and also quite human. 



Animals can do all sorts of clever things we can do. They can use tools, problem solve, communicate, cooperate, exhibit curiosity, plan for the future, and although we can’t know for sure, many animals certainly act as though they feel emotions: loneliness, fear, joy. Apes have even been taught to use language to speak to us humans. It’s a sort of sign language that they’ve used to do everything from answer questions to express emotion, or even produce novel thoughts. Unlike any other animal, these apes are able to understand language and form responses at the level of a two and a half-year-old child. But there’s something that no signing ape has ever done. No ape has ever asked a question. Joseph Jordania’s “Who Asked the First Question?” is a great read on this topic, and it’s available for free online. For as long as we’ve been able to use sign language to communicate with apes, they have never wondered out loud about anything that we might know that they don’t.



Of course, this does not mean that apes and plenty of other animals aren’t curious. They obviously are. But what it suggests is that they lack a theory of mind, an understanding that other people have separate minds, that they have knowledge, access to information, that you don’t have. Even us humans aren’t born with a theory of mind, and there’s a famous experiment to test when a human child first develops a theory of mind. It is called the “Sally-Anne test”. During the test, the researchers tell the children a story about Sally and Anne. Sally and Anne have a box and a basket in their room. They also happen to have a delicious cookie. Sally takes the cookie and puts it inside the box, and then Sally leaves the room. While Sally is gone, Anne takes the cookie out of the box and puts it in the basket. When Sally comes back, the researchers ask the children where Sally will look for the cookie. Obviously, Sally will look in the box. That’s where she left it, and she has no way of knowing what Anne did while she was away. But until about the age of four, the children insist that Sally will look in the basket, because after all, that’s where the cookie is. The child saw Anne move the cookie, so why wouldn’t Sally also know? Young children fail to realize that Sally’s mental representation of the situation, her access to information, can be different than their own. And apes that know sign language but never ask us questions are doing the same thing. They’re failing to realize that other individuals have different cognitive abilities and can be used as sources of information. 



So, we are all alone with our perceptions. We are alone in our own minds. We can both agree that chocolate tastes good, but I cannot climb into your consciousness and experience what chocolate tastes like to you. I can never know if my red looks the same as your red. So stay human. Stay curious, and let the entire world know that you are. And as always, thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. Skeletons are scary and spooky, but you know what else is? Teenagers. Their attitude, the way they dress and the music they listen to. Can you even call it music? Pff, kids these days.



But what are kids these days? What's with all the concern and what's a generation? Why do we think that coevals, groups of people of roughly the same age, act so much alike?



The sheer number of articles and papers and internet posts published daily comparing then and now, both sincerely and ironically, is astonishing. We can't seem to get enough about kids these days and just how different and awesome it was to be a kid back in the good old days. Generational labels make human history look ordered and discreet, instead of scary and messy. They also have a delightfully suspicious tendency to flatter those using them. George Orwell put it well. "Every generation imagines itself to be more intelligent than the one that went before it, and wiser than the one that comes after it." There's a name for this sentiment. Juvenoia.



Sociologist David Finkelhor coined the term. It means "an exaggerated fear about the things that influence kids these days." Juvenoia is a concerned disappointment that because of iPhones or the Internet or TV or rock music or those pesky horseless carriages the world just isn't fit for kids like it used to be. Generational conflict really has been going on for that long. After all, "honor thy [your] father and thy [your] mother" was an ancient commandant for a reason. In the 4th century BC, Aristotle remarked that youth’s mistakes are due to excess and vehemence, they think they know everything. Here's an engraving from 1627 admonishing the 'now,' compared to the ways of 'old.' In the early 1900's Romain Rolland complained that the new generation of young people were, quote, "passionately in love with pleasure and violent games, easily duped." New people and the direction society is headed in has always been seen with some disapproval. Xkcd famously collected a brief history of juicy examples.



In 1871, the Sunday Magazine published a line that may as well have been written today about texting. "Now we fire off a multitude of rapid and short notes, instead of sitting down to have a good talk over a real sheet of paper." And the Journal of Education in 1907 lamented that at a modern family gathering, silent around the fire, each individual has his head buried in his favorite magazine. The point is there's nothing new under the Sun. Not even the Sun, in fact. The Sun is believed to be a third generation star. This constant cycle of generation clashing can sometimes sound like a broken record. Are these commentaries really providing insight into the minds of future leaders or prematurely judging a coeval based on how it acts as teens? Despite the incessant concerns otherwise, the proverbial 'kids these days' seem to be better off than ever before. Drug use is down, exercising is up, math and writing proficiency have increased, crimes committed by young people have decreased, hate comments reported by children have dropped, the number of 9th to 12th graders who have been in fights has dropped, and the number of teens who fear attacks at school has dropped. But still, juvenoia persists. But why?



Well, it kinda makes sense. I mean, children are the future of a species, so it's reasonable to assume that nature would select for features in a species that cause adult members to prefer the way they were raised and distrust anything different. After all, parents, by definition, were a reproductive success for the species. They made new members. So whatever choices and influences brought them to that point must have been good enough. Any deviation from that could be a problem. So worrying about the young may have been naturally selected, just like eyes and fingers and breathing air and pooping. But here's the thing. Our brains don't accurately remember the past or apply memories fairly or rationally. That kind of thinking has a plethora of interesting causes.



First, at a social level, concerns for and about the youth are often exaggerated, because exaggerating is effective. You'll generate greater mobilization around the cause if you can convince people that we're on the cusp of a crisis here, folks. Also, our increasingly connected world means more potential contacts with people outside the family, the tribe, the neighborhood. Even though juvenile problems often involve people the juvenile already knows, stranger danger is a more powerful fear. "My kids have good friends, who are good influences, so why should I worry?" can be replaced today with "even so, people you don't know are threats, so worry."



Other reasons for juvenoia are personal and often it's not so much the world that's changed, it's you. Are drivers today really worse than they were when you were young or do you simply have new responsibilities and experience that makes you more aware of dangers that were always there? We remember the past abstractly. There just isn't enough room in our brains or a vital need for complete voracity when recalling things. Thus, we are more likely to remember the general way we felt in the past, without the petty annoyances, more salient still, for the present.



Secondly, loss aversion and the endowment effect. People perceive a loss as greater than an equal gain. In one famous study, when asked how much they would pay for a coffee mug, people gave prices that were significantly lower than what people given the mug first said they'd be willing to sell it for. This may play a role in how we value what we already have - our memories and favorites - over what's new. There's even neuroscience backing up why new stuff seems so bad to you. It's called the reminiscence bump. Storage of autobiographical memories, memories about yourself, increases during times of change. Incidentally, this is why you remember exciting things as lasting longer than they really did, but rarely remember times of boredom in detail. I've discussed before the ways in which this causes us to feel like time slowed down during particularly quick but significant events.



Anyway, adolescence and early adulthood, particularly ages 10 to 30, are major times of change. Many important things happened during those years that define your identity. So, it's no surprise that along with things that have happened recently, memories from this bump period are greater in number and more emotional. The books and songs and movies and slang words and behaviour you loved and used during this time correlates quite well with what you will, when you're older, remember the most fondly.



As we can see, juvenoia is natural. In fact, a healthy dose of it is important. There are plenty of things we should be fired up about improving. What's sometimes lost though, when explaining that juvenoia occurs in every generation is the fact that the nature of juvenoia hasn't always been the same. The generation gap of antiquity, or of the 1300's, wasn't the same as it is today. The more rapid speed of change may be one reason, but another is the appearance of a new type of creature around the turn of the last century - the teenager. The word teenager wasn't even used as a stage of life until 1922. John Savage's 'Teenage' is a fantastic read on how human society sort of accidentally invented the teenager.



You see, as factories generated new unskilled jobs, young people could acquire something neat - their own money. Suddenly, marketers realized that products could be made for the youth. They were no longer stuck with what their parents decided on. Also, the surge in immigration at the time highlighted for a new generation the view that identity wasn't something you're stuck with. It's fluid, personal, decided.



Furthermore, calls for compulsory education around that time, that is, making it the law that children go to school, further solidified the segmented identity of children by forcing them out of the world at large and into common places surrounded mainly by their coevals. In that environment they could develop behaviors and opinions and culture shared just with themselves. Compulsory schooling also increased literacy in adolescence, which gave them all the more power to hear stories written for them and about them in books they could buy with their own money. Kids these days suddenly weren't just young humans waiting for life experience, they were separate beings with their own culture and voice. A fact that caused juvenoia to change from the edible skirmishes of the past into the full- fledged panics we know and love today. This brings us to a bigger question, though.



Sure, you may say, that makes sense, but even someone who didn't grow up in this society could plainly see that in the old days culture wasn't as dumbed down as it is today. Things used to be made by the elites, for the elites. Now they're made for the masses who demand sensational atavistic pablum instead of rational critical thought, like scholars, and, well, you know, me. Those examples sure are convincing but the plural of anecdote isn't data. You can pick different examples and argue the opposite point. Mozart wrote poems about farts. There is amazing work and there is simple work made at all times in history. In fact, as



Steven Johnson points out in "Everything Bad is Good for You," if anything, when given the chance to buy or participate as they choose, the tendency we find in humans is a preference for more cognitive demands, for smarter entertainment. What it takes to keep up with the increasing density and intricacies of narratives in media these days is impressive. To be fair, of course, beneath the stimulating organization there is no substance anymore, right? I mean, here's what one noted critic said of today's easy brainless mass culture. "We do not turn over the pages in search of thought, delicate psychological observation, grace of style, charm of composition, but we enjoy them like children at play laughing and crying at the images before us." Wait, sorry, that's something literary critic G. H. Lewes wrote about Dickens in 1872.



The point is, taste is subjective. Art to 1 percent is garbage to another. You may dislike the language or violence or morals depicted on TV today, but there's no denying the fact that entertainment, including popular entertainment, is requiring more and more thinking on the viewers' part than ever before. Johnson created this visual comparing narrative threads in episodes of different TV shows over time, and this shouldn't be surprising. Our brains crave stimulation. A lump that sits and stares into space isn't naturally selected form in the same way as a brain that learns and synthesizes and organizes. Now that entertainment can be made for niche audiences and watched and re-watched on demand and discussed ad nauseam online that natural desire can be sated by media. Johnson goes so far as to say that reruns have made us smarter. They've enabled entertainment to be made that rewards being watched and thought about over and over again. The names and stories and relationships and dramas people today have to keep straight in their heads to be functioning consumers of modern media are impressive by historical standards and affect more of us than ever before. Johnson points out that in his time Dickens was only read by 0.25% of his country's population, while today innovative shows like 'The West Wing' or 'The Simpsons' easily reach twenty times that proportion. Okay, but how about this?



Where are the Mozart's and Dostoyevsky's of today? Well, they're probably here, but the reputation of Dostoyevsky is built by time, something the judgments of contemporary artists haven't had enough of yet. Finally, when it comes to judging works that merely tease the base emotions let's not forget the quote from Unamuno I've discussed before. "More often have I seen a cat reason than laugh or weep." Cats and humans are curious and can problem solve, but only humans can laugh at fart videos. So, what really ought we be treasuring? There's a problem here, though.



Although writers like Johnson have been able to put forth convincing arguments that movies and TV have been serving more and more cognitive complexity, they've failed to find the same evidence for pop music. Nearly all studies on the subject have found that, unlike other forms of popular media, pop music has, in fact, become, since the 1950's, less complex in its structure and more homogeneous. Mathematically speaking, more pop songs today sound alike than they used to. What's up with that, music? Well, here's the thing. Pop music is just one type of music being made today and it's role, what its its listeners want from it and who they are are much more specific than the wider spectrum of genres a movie theater or Netflix caters. A pop song needs to provoke quick mood, stick into your head and fit anticipation and pay off into a fairly regular amount of time. There are only so many ways to do that.



So, perhaps, pop music producers have simply gotten better at scratching the specific itch they're challenged to scratch. I mean, imagine criticizing doctors for using penicillin nowadays. Uh, back in the good old days treatment was innovative. There were leeches and onion plasters, amputation and good luck charms. Now it's all just penicillin, penicillin, penicillin. It's all the same. Criticizing popular music for all sounding the same ignores the sameness of every pop song's goal. But what about generations? What are they exactly?



I mean, humans don't have babies all at once every twenty years or so. New people just keep showing up, about four more every second. But that said, there are biological changes humans go through as they grow and age, roughly creating a few life stages. Alright? Now, this list of generations goes all the way back to the mid 1400's. It applies mainly to the western world, especially the US, and is the work of William Strauss and Neil Howe, whose landmark 1991 book "Generations," contains one of the most influential and ambitious generation theories of our time. These are the guys who coined term 'millennial,' by the way. They set forth and have continued to expand a theory that society follows a predictable cycle of moves, each lasting about 20 years - about how long it takes for everyone in a life stage to move on to the next. The social mood and the common life stage a coeval experiences it during are what distinguishes one generation from the next. Strauss and Howe call each social mood a turning. A turning describes the way society will act, by either establishing, accepting, challenging or fracturing in lieu of established customs. To illustrate the cycle, let's start just after the American Civil War, in the so-called Gilded Age.



Here we find American society in the first turning, what they call a "high." This is a twenty-year period when society is largely in agreement about the direction it wants to go in, because it recently coalesced in the face of a crisis. Institutions are strong and thus young adults are cautious and conformist. But then people tire of social discipline and call for reform, a period of awakening occurs. The majority consensus is attacked in the name of greater and broader individual autonomy. The distrust in institutions left in the wake of an awakening leads to the next turning, an unraveling, where in place of broad cultural identity, moral crusades polarize society over what should come next.



Finally, a renewed interest in consensus that responds to crisis by banding together occurs. Society's mood shifts to a belief that coalescing and building together are the answer. The cycle then starts again with a high - the majority agrees on society's directions and institutions strengthened during the crisis until people tire of this majority structure and an awakening leaves those institutions week and armed with less public consensus. This is followed by an unraveling, where individuals polarize over moral issues and the youth, raised in the previous two atomizing moods, feel alienated. Which brings us to, well, today. Strauss- Haus theory, if true, tells us that this will be an era where society will band together and build institutions from the ground up in the face of crisis. It's not clear what that crisis will be, but if their theory has predictive power, the climax of that crisis will occur in 2025.



The whole theory is a great way to learn about you as history. Al Gore once even gave a copy of Generations to every member of Congress. But it is unscientific and unfalsifiable. You can find a pattern in anything if you pick and choose the right examples. As for the usefulness of its generalizations, well, Philip Bump points out that the US Census Bureau only recognizes one official distinguishable generation. Baby boomers. Do you think you are a Generation X, a millennial, Generation Z? Well, that's fine, he says, you call yourself whatever you want. It's all made up. The baby boomers are a cohort, significant in that no matter where they were born or who they are, their size alone determines a lot about their path. But other population segments, based solely on birth year, just don't mean much.



A more useful way to divide them into cohorts might focus on some other less age-related trade that correlates better with behavior. Wealth, region, sexuality, etc. Regardless of its accuracy there's one thing generational theory and its critics do at least agree on. People change as they age and the larger society surrounding people influences the degree to which generations feel conflict. So, generational thinking is a kind of guidance. It's one that helps take us on a journey, manned by an ever changing and changing crew. Some crews are different than others, for sure, and you need worry and concern to stay safe. But at the end of the day, it's still the same boat and the same waters. Generations and juvenoia are like what Picasso said about art - they are lies that tell the truth. And as always, thanks for watching.



Here's a bonus. While doing research I found a website that will allow you to find what word was first used the year you were born. It's pretty cool. It's also a DONG. That's right, something you can do online now, guys. In fact, here's some DONG news for you.



The shows that you know and love on the Vsauce network now have their own home, where they can flourish and be who they want to be. The channel's name is DONG. Go over there right now to check it out, we've got some cool things from the Internet. That's what DONG's all about. It's an Internet Safari. It's neat things that we all find as we research for these episodes. So I'll see you over there on DONG. And thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. This is Earth as seen from Saturn. That is us right there. And if you look closely, ok, see this little protuberance? That's the Moon. This image was taken by the Cassini spacecraft on July 19th, 2013 at 21:27 Coordinated Universal Time. The thing is, NASA gave the public advanced warning of when it would be taken, which means that this image of Earth was the first ever taken from space that some people on Earth were actually posing for.



Our planet looks so small, insignificant, fragile. I recently attended the premiere of Sky 1's upcoming "You, Me and the Apocalypse" with some cool YouTubers and it got me thinking. In the show, the characters find out that they're only 34 days left before a comet smashes into Earth that's likely to end humanity. They all react in different interesting ways, but what would I do if I found out that there were only 34 days of human history left? Ok, my first priority would be to get back to America to be with my family. But after that? I don't really have a bucket list. Except that is exactly what I would want to spend my last few weeks doing. Making a list to put in a bucket that I would then send far out into space away from Earth's impending vaporization.



The list would contain information about us, all Earthlings. So that if libraries and monuments and YouTube videos were all destroyed, a record would still exist somewhere of what and who we were. Like a stone thrown into a lake, the ripples your life causes last long after you vanish, the tree you planted is climbed by future generations, the books you donated inform future readers. But what if it's not just your stone that vanishes, but the entire pond? Perhaps it's arrogance or vanity, but getting cosmic messages in a bottle out there, before the end, diversifies our archive and gives a better chance for future alien visitors, or whatever is left of humanity, to find out that we were once here, to show what we learned. Maybe even to warn future life forms of what we did or what we didn't prepare for. We have already sent some messages about humanity out there, beyond Earth, and if Earth is completely destroyed, those messages will be all that's left of us. What are they?



Ok, first things first. How do you write something for the future? I mean, the distant future. The message might not be found for millions of years or billions. It might be discovered by an audience that's completely different, not only in language, but in senses? What if they can't see or hear or feel or taste or smell like we do, or at all. What if their bodies destroy the very material we write the message on? What language do you even write it in? Well, in general, math and physics, which are believed to be the same everywhere in the universe, have been what we write outer space bound messages in.



Like the Arecibo message, written by Frank Drake, Carl Sagan and others, which was blasted towards the M13 star cluster in 1974. It's composed of a semi prime number of binary digits conveying some info about us and it should reach the center-ish of the M13 cluster in about 25,000 years, at which point, if something intelligent lives there and detects it, they can respond and their response will return to us another 25,000 years later. We won't be around for that.



But Earth has also been broadcasting its radio and TV signals into space. Currently it's about 200 light-years in diameter. Compared to the Milky Way, it's about this big. Aliens within that bubble could tune in and listen to programs we sent out through our airwaves, but these signals thin out as the bubble expands. Across very large distances they may be essentially impossible to tune into.



Maybe a physical time capsule would be more permanent, but it can't be buried on Earth if Earth is about to be ravaged. A time capsule in orbit might be smart, like LAGEOS-1, a satellite put into orbit in 1976 that allows for very precise laser measurements of positions on Earth, but also contains a plaque designed by Carl Sagan, upon which is written the numbers 1 to 10 in the binary, and the arrangement of the Earth's continents 250 million years ago, today and their estimated arrangement in 8.4 million years, which is how long we believe the satellite's orbit will be stable. Drag caused by the thin atmosphere up where it orbits and influences like solar activity will eventually cause it to fall back down to Earth, but its plaque will serve as a time capsule - a message from us today to whatever happens to be alive or intelligent here on Earth 8 million years in the future. To put that in perspective, the pyramids were only built about 5,000 years ago. 8 million years ago, there weren't even humans on the Earth. The latest common ancestor of humans and chimpanzees was around though. 8 million years from today, when LAGEOS returns, what will intelligent life on Earth look like? If Earth's surface is barren of life at that point, LAGEOS-1 will be alone.



But what about satellites in geostationary orbits? These orbits are far enough out that they're much safer from atmospheric drag and could remain above Earth much much longer than satellites like LAGEOS. These satellites are our pyramids. They're smaller than monuments built by past civilizations, but impervious to anything that might go wrong on the less stable surface of our planet. If alien archaeologists come by in a billion years or so, these satellites may be what their alien encyclopedias use as the picture for the humans article. So far we have erected about 450 of these geostationary monuments. When such a satellite wears down and ceases to be operational, it takes a lot of energy to slow it down so it can move out of the way and fall to Earth to burn up in the atmosphere. So instead, they're usually pushed into what's known as a graveyard orbit. A shell around the planet where they can be part without interfering with important operational satellites. It's fitting that we call these graveyard orbits because tombs are often the most stunning things we have from previous civilizations. These graveyard orbits are tombs in a way. Not for kings, but for machines. Junkyards that will out-exist the very societies and people they so largely define. Luckily, a few contain more than just our craftsmanship.



They also contain a record, like EchoStar XVI, a communications satellite launched into geostationary orbit in 2012. Aboard it is a silicon disc created by artist Trevor Paglen, containing 100 images of Earth and Earthlings. Now, unlike LAGEOS, EchoStar XVI will likely remain in orbit for billions of years, safe from discord and change down here. But here's the thing. What if our entire solar system is lost? Or what if life out there doesn't decide to ever visit our system?



Well, in that case, we have sent interstellar messages. At this moment, so far, there are 11 distinct human made things on trajectories out of the solar system into interstellar space. They're all related to five probes. Pioneer 10, Pioneer 11, Voyager 1 and Voyager 2 and New Horizons, the thing that recently made a Pluto flyby. These objects are our most distant hellos. Over the next ten thousand, million, billion years, they'll pass close enough to other star systems, maybe even planets, to possibly be discovered by other intelligent life forms. We had the foresight to include special messages on these probes.



The Pioneer plaques are attached to Pioneer 10 and 11, which launched in the early 1970's, were the first human-made objects to ever be sent on a trajectoryto not just leave Earth, but to leave the solar system entirely. If discovered by other life out there, these plaques, designed by Frank Drake and Carl Sagan, could be our first chance to say "hello, we exist," or, depending on how long humanity lasts, our only chance to say "Hello, we existed. This is what we were." But will the plaques makes sense to aliens? Many human scientists have had trouble deciphering their meaning, but here's what they say.



At the bottom is a map of our solar system with a path showing the Pioneer probe itself and where it came from. This element has been particularly criticized for being human centric. I mean, an arrow? Who's to say aliens will know that this depicts a path and not some structure in our solar system? Also, it's an arrow. Arrows might convey this way only two civilizations that hunted or developed pointy projectiles. Anyway. Up here, we define units. You can't tell aliens about humans or Earth by using seconds, kilometres or light years, because we made those measurements up. Instead, the plaque uses hyperfine transitions to communicate distances and time.



The hope is that curious intelligent life forms who find this will understand that this is a hydrogen atom - one proton, one electron. Hydrogen is the most abundant element in the universe, so hopefully its properties will be a common point of understanding. Now sometimes, if you've got enough hydrogen around, you can catch atoms in the heap transitioning between particles with parallel spins and antiparallel spins. Now, whenever this transition happens, electromagnetic radiation is released with a period of about 2.7 nanoseconds and a wavelength of about 21 centimetres. It's hoped that aliens equate this tick mark with these two units of measurement. For example, the woman is said to be, in binary, eight of these units tall. 21 centimetres times eight is a 168 centimetres. You would think that putting the probe itself behind them for scale would be enough to show our size, but having both might help solidify the connection.



This diagram is meant to show where we live. We are in the middle. The direction and proportional length of the lines show where distant pulsars are. A tick- mark near the ends of each line shows the third direction, behind or in front of the plaque, that the line should go in. In binary, next to each line is the period of each pulsar. Again, in the time units described above. If aliens make this connection they could possibly match the periods with the correct pulsars in real life, triangulate our position and come say hello. Also, since the period of a pulsar changes over time, they could use our observed periods to date back how long ago this plaque was made. So that's how the plaque works. It kind of feels like we're just sending out a bunch of science homework to space, but how else can you find common ground with something that might not resemble you in any way you could even imagine?



Despite heading out first, the Pioneer plaques are not the first physical messages made by us to go interstellar. That title belongs to Voyager 1. Currently moving at 17 km/s, it is the most far-out thing humans have ever made. Literally. In about 40,000 years, both Voyagers will pass within less than two light years of other stars. If aliens find them, or if future humans find them, a golden record is attached to both that contains information about humanity. The record is made of gold plated copper with an aluminum cover, containing some uranium 238. Given its half-life, smart aliens could analyze it to determine how long ago the record was made.



On the record is the inscription to the makers of music All Worlds All Times. The record contains 116 images, as well as audio and video recordings of humans, animals, songs and greetings in fifty five languages. Printed on the record are instructions for playback and info about us. The hydrogen hyperfine transition unit definition and the Pulsar map, included on the Pioneer plaques, but the record also comes with a stylus and platter to play it. Instructions for using the stylus are on the record. This is a picture of the stylus being used correctly and then in binary using the transition units, the record tells the aliens that the stylus should go around the record once every 3.6 seconds to play back correctly and in total should take about one hour to do so. For the video portion, instructions are given in a circle. The first video image is displayed, so they know they did it right. Also included on the record is a message from then-president of the United States, Jimmy Carter.There's something vulnerable about the message. It's delivered to an unknown recipient, like when someone in a horror movie asks into the darkness "Is anyone there? Hello?" This is what it says. "We cast this message into the cosmos. It is likely to survive a billion years into the future, when our civilization is profoundly altered and the surface of the Earth may be vastly changed. Of the two hundred billion stars in the Milky Way galaxy, some, perhaps many, may have inhabited planets and space faring civilizations. If one such civilization intercepts Voyager and can understand these recorded contents, here is our message. This is a present from a small, distant world. A token of our sounds, our science, our images, our music, our thoughts and our feelings. We are attempting to survive our time, so we may live into yours. We hope someday, having solved the problems we face, to join a community of galactic civilizations. This record represents our hope and our determination and our goodwill in a vast and awesome universe."



Are these messages enough? Should we send more? Well, why not send everything? We could, if we sent the Library of Babel. It's a website built by Jonathan Basile that currently offers everything that has been or could be written. Seriously. Divided into pages, it is built to produce and locate on demand any 3200 character combination of English letters and the comma, space and period. Basile has organized it all into hexagon-shaped rooms, each with four walls of books containing five shelves with 32 volumes of four hundred and ten pages each. Everything's arranged in a pseudo-random fashion, so browsing the online library feels like a treasure hunt.



Here's how it works. Each page is given a unique sequential page number in base 10. The text on each page is encased inside this number. An algorithm Basile created uses the page number as a seed to generate a unique big number. That output is then converted into base 29 so that it can be represented using every letter in the English alphabet as well as the comma, the space and the period. This is what you see on the page. Basile has made sure that the algorithm will produce every possible combination and the same page number will create the same output every time. Which means that what is on each page is already predetermined. So, in a way, every page already exists. It only needs to be looked up. And here is the really mind blowing thing. The contents of a page can be converted to base 10, sent through the inverted algorithm and turned into the exact page number they're found on. It's a truly eerie experience, because you can find the permanent location for any 3200 character text.



 You can find in this library the description of your birth, every possible description of your death, every poem, every joke, every lie, anything that could be said can be found on this site. This thing blurs the line between invention and discovery. Did you really discover or invent that thing if its description already existed? 10 to the 5,000 different pages are offered by the Library of Babel. In comparison, there are only 10 to the 80 atoms in the observable universe. I searched for what I've just said and sure enough in this hexagon, on this wall, this shelf and this volume on this page it's there. Hello.



But deep down, we feel like there's a difference between this program permuting something unknowingly and a person actually meaning it, intending it, saying it because they wanted to with agency. We use a finite number of symbols to say things. For that reason, a library of every finite combination of those symbols can be made, but just because it can be made doesn't need it has been said. That is the power we have. Perhaps you and I were born too late to explore the world and too early in history to explore the stars, but we were born at just the right time, which is pretty much all times ever to explore language. To explore what can be said. What should be said. What should we send out to space. What that can't be said will you be the first to say? And as always, thanks for watching.



To watch the trailer of You, Me and the Apocalypse click right up there and to see some other cool things YouTubers have done inspired by the series. Well, you can also click right over there. Colin Furze is building a giant apocalyptic bunker in his backyard, like for real. It's huge. Also, The Dictionary of Obscure Sorrows has weight in. Check those out and thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. This picture is about a year and a half old. But the pyramids themselves are much older than that. How much older? Well, think of it this way. The Pyramids of Giza were as old to the ancient Romans as the ancient Romans are to us. When the pyramids were being built there were woolly mammoths living on Wrangel Island. That's pretty old. But don't get too impressed. 



We often learn about the past in units, separate chapters, which distracts from the fact that many chapters aren't just nearer each other than you might think, they are often literally written on top of one another. Anne Frank and Martin Luther King Jr. were born in the exact same year. By the late 1960s humans had come a long way. We've become a spacefaring species. But while we were sending the first probes to the Moon and Venus and Mars, it was still illegal for a black person and a white person to marry in 16 states. The guillotine seems like a macabre artefact from bygone days but it was last used by France to officially behead a criminal the year Star Wars came out. While General Custer was fighting native tribes on the American frontier, the Brooklyn Bridge was being build. And there were people alive then who would later watch the Moon landing on television. We went from Custer's last stand to Armstrong's first steps within the span of a single human life. 



But all of these stories, from the pyramids to Julius Caesar to you watching this video right now, belong to an incredibly thin section in the book of human history. Compared to what human life has mainly been like here on earth, our current societies are weird. Weird is also an acronym used by Jared Diamond in his new book "The World Until Yesterday." Jared Diamond wrote "Guns, Germs, and Steel," "Collapse" and he's here with me to talk about weird. Weird, w-e-i-r-d, is an acronym for "western, educated, industrial, rich and democratic." 



When we talk about human nature we're really talking about a narrow slice of society. Traditional societies were everybody in the world, from the beginning of human evolution 6 million years ago until within the last 10,000 years for the first time we began encountering strangers and then we developed writing and then we acquired kings. All of these things that we take for granted are matter of the last 5,000 - 10,000 years. Or the Internet or the media, they're a matter of the last few decades. The relative recency of weird societies in the speed with which information and knowledge increases means that not that long ago we thought some pretty strange things. For instance, in 1903 The New York Times predicted that building a flying machine would be possible in 1 to 10 million years. Later that very same year the Wright brothers flew at Kitty Hawk. In 1908 it was said that no flying machine will ever fly from New York to Paris. Who made that foolish prediction? One of the Wright brothers. In 1962 the Decca recording company passed on a young rock band, saying "we don't like their sound and guitar music is on the way out." The band in question was the Beatles. Keith Moon and John Entwistle were said to have remarked about a band called The New Yardbirds, "that band so ill-conceived it will sink like a balloon or a zeppelin made out of lead." Well, Jimmy Page was not deterred. In fact, he took that phrase and made it his band's new name. He removed the A, so it wouldn't be pronounced "lead," and that's where we got Led Zeppelin. 



There is evidence the Nazis weren't entirely convinced Earth was a globe we lived on the outside of. Instead, they figured the Earth's continents were actually aligning on the inside of a hollow concave surface with the stars and moon and planets in the middle. 



Seriously. 



As the story goes, Doctor Heinz Fischer was sent to Rügen Island to spy on the British. Now, the curvature of the earth would have made this impossible. But thinking the earth was shaped like this, they pointed their telescopes up at a 45 degree angle. Needless to say, the experiment didn't work. The Eiffel Tower's inauguration and the Wall Street Journal and "Starry Night" and Coca-Cola and Nintendo and Adolf Hitler all began in 1889. 



As did a guy named Thomas Midgley Jr. Celebrated in his time, Midgley's legacy has since been tarnished by the negative consequences of his inventions. His list of contributions to society is impressively disastrous. In the late twenties, Midgley synthesized the first chloro-fluorocarbons - CFC - for which he won the Society of Chemical Industry's Perkin medal. Only later did we realize all of those tons of CFCs we were emitting were eating away 4% of our atmosphere's protective ozone layer every decade. Like a virus, creating a wound, unlikely to completely heal until me and you have long been dead. In the early 1920s Midgley discovered that by adding Tetraethyllead to gasoline engine knocking could be reduced. The American Chemical Society gave him the 1923 Nichols' medal for the discovery. There were other, safer alternatives but General Motors jointly owned a patent on Tetraethyllead with Midgley. They could make a profit on it, so they advertised it as the best option and almost immediately nearly every motor vehicle on earth was spewing lead into our atmosphere and soil, which put it in our blood, lead poisoning for decades. 



Currently, the reference for healthy children is a blood lead content of less than 5 micrograms per decilitre. After the popularization of Midgley's leaded gasoline, 88% of children in America had double that amount of lead in their blood. When leaded gasoline was finally phased out in the 1970s, that percentage fell to 9%. Lead is a neuro toxin. Even light exposure, like that caused by Midgley's invention, can cause a decrease in intelligence and in increase in anti-social behaviour. Fordham University found that in young adults the best predictor of delinquent and violent behavior is literally the lead content of their blood. Chillingly, the rise and fall of violent crimes by juveniles in the 20th century tracks significantly closely with the rise and fall of lead in their blood when they were preschoolers - all over the world. 



Historian J. R. McNeill remarked that Midgley had more impact on the atmosphere than any other single organism in Earth's history. Midgley's final invention was even worse. Well, for him. In 1940 he contracted polio. To help his friends and family lift him from bed he designed an intricate system of ropes and pulleys. As was the story of his life, the invention seemed brilliant at first, but four years later he became accidentally entangled in the ropes and his own invention strangled him to death. Midgley's life took up 0.55% of human history. And, roughly speaking, yours will too. 



To put that in perspective, let's time travel, starting right now. We begin 100,000 years ago - the beginning of modern humans. We are moving forward in time an entire millennia, a thousand years, every second. As you can see, not much is changing. 



Our modern world will briefly flash at the end. That's all it is. That's all it's been. So be careful not to miss it. And as always, thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. Now, one of my favorite treats of the holiday season is Gabriel's cake. It's a super solid based on Gabriel's Horn that you can make right at home, as long as your home is infinitely large. Okay, all right. Now, the first thing you want to do is bake a cake. I prepared this cake earlier. It's a real beautiful cake. It's a little large, but I bet you I could eat that whole thing in a day if I tried. Am I right? Okay, now the second step is cut the cake in half. Notice that while I cut this cake in half you don't add or create any new cake but the surface area of the cake has increased. It used to be completely covered and now we've got two regions on the inside that aren't covered. The next step is to cut this half, one of the halves, in half. Here we go. Again, the volume of cake on the table is the same as it was in the beginning but the surface area is going up. As you may be able to guess, the next step is going to be to cut one of these quarters of the original cake in half. It's getting pretty thin, but all you have to do is keep this up - cutting halves in half and half and half and half forever and once you've done that, well, you're almost there. When you finish cutting, stack the halves on top of one another in order, like this, to create a beautiful tiered dessert. Because you have an infinite number of thinner and thinner slices when you're done stacking them the cake's vertical height will be endless.



Such a cake has an interesting mathematical property. Its volume, the amount of cake it contains, is clearly no different than the amount you started with, but its surface area is infinite. It's a cake you can eat but not frost. You would need an infinite amount of frosting to cover the whole thing with a uniform coat. An object with finite volume but infinite surface area doesn't need to be endlessly tall, by the way. There are bounded super solids, like this cube, with an infinite number of smaller and smaller circular holes. Of course, building these objects in the real world comes with some obvious difficulties. One is the fact that the amount of steps required to complete the construction of Gabriel's cake or any other super solid is literally infinitely many. And infinity isn't a number that, you know, you eventually get to. It means unending. There will always be a next step, another piece to slice in half again. By definition, an infinite sequence of tasks has no last task. So you could never finish making a super solid. Or could you? Enter the Supertask.



What if instead of taking the same amount of time to complete each step we accelerated as we worked and did each step in half the time as the last? For example, let's say you want to make Gabriel's cake in just two minutes. That's easy. First, cut the original cake in half and then wait a minute before making the second cut. Wait half a minute before making the third, a quarter of a minute before the 4th and so on. Always waiting for half of what's left to pass before cutting again. Since you can keep dividing in half forever, there will always be another step. Infinite actions within a finite amount of time is a supertask. The strange thing, of course, is that while supertasking no matter how many steps you've already completed there will always be an infinite amount of steps ahead. But yet, when time's up, you will have finished all of them.



What we've just described is similar to Zeno's famous paradox of the dichotomy in which the Greek hero Achilles runs a short race that he can clearly finish, I mean, people finish races all the time, but how exactly it's finished is a mystery. Because first Achilles must cover half of the race's distance and then half of what's left and half again and again and again. Since there will always be another half way point to reach, the number of subdivisions infinite, there is no final destination, after which Achilles' next stop is the finish line. But yet Achilles obviously can finish. He somehow reaches them all despite the fact that during his journey he always has an infinite number of steps left to reach. Think about it this way.



What if at each half way step along the way Achilles is required to hold up a flag. At even steps a blue flag and at odd steps a red flag. Blue, red, blue, red, blue, red, the flags will alternate faster and faster. But when Achilles finishes the race, which flag will he be holding up? To be holding up either flag would seem to suggest that a largest number not only exists, but is either even or odd. Now, to this it is often suggested that the real problem here is that Achilles isn't actually doing infinitely many things. He's just taking a few strides or whatever.



A true supertask requires doing infinitely many distinct actions. Fair enough. But even if Achilles ran the race in a staccato fashion, stopping at each half way point for half as long as he waited to the last one, the scenario would still be sensical and logical and would complete in a finite amount of time. It has even been shown how a staccato runner could move to avoid discontinuous velocities and accelerations. The point is, fine, infinite distinct actions are at least logically possible to do in a finite amount of time, but only logically. Only as a mathematical abstraction. Supertasks are obviously just products of our imagination. Because in the real world there must be some smallest amount of space and/or time that cannot be meaningfully divided in half again. Once you're within one of these distances of the finish line, the only possible next position you can have is the finish line. That certainly seems fair. After all, it's believed that in reality there is a smallest meaningful distance - the Planck length.



Any interactions between or observations of particles more accurate than this span make no sense within physics as we understand it today. And the time it would take to travel a Planck length at the fastest speed possible, the speed of light, is the Planck time. So these seem to be the briefest pieces of space and time any known force in the universe could act across. Whether that force is gravity or a force from Achilles' leg muscles. But does that resolve the crisis or is it just a way of avoiding the problem altogether by saying "it doesn't matter, because it won't be on the test?" As John Earman has complained, it seems to me unattractive to make the truth of mathematical statements depend on the contingencies of space-time structure. Maybe Zeno's dichotomy tells us less about motion and time and space and truth than it does about our impressive abilities to confuse ourselves. And that's what makes supertasks important.



They're a contact point in uneasy handshake of sorts between the universe we live in and the brains inside us. Let's play around with some. They're not all the same. Some converge, like Zeno's dichotomy, others converge but in ways you may not expect and still others refuse to cooperate in any way. They diverge. A great example of a supertask whose behavior diverges is Thomson's lamp, a famous supertask devised by James F. Thomson. Imagine a lamp that can be turned on and off as quickly as you desire. What would happen if you turned such a lamp on and off Zenoianly? Well, let's find out.



Just set a timer and turn the lamp on. Then wait one minute and turn it off. After half a minute turn it back on and then off again after a quarter of a minute, on again after an eighth of a minute and so on. Waiting half as long to flip the switch each time as you previously did. Now, as the number of switching grows without bound, the total time elapsed approaches just two minutes. The lamp will be turned on and off an infinite number of times in just two minutes. So after two minutes, will the lamp be on or off? Well, by the definition of infinity, there is no last step. There is never a switching not followed by another. So the lamp can't be on, because whenever it's turned on, it is immediately turned off in the next step and it can't be off because whenever it's turned off it's turned on right afterwards. What's the answer? It's easy to say something about a supertask when its partial sums converge, but when they just oscillate back and forth forever... Hmmmm... Zeno objects do this.



Imagine building a meter-high cube. As with all supertasks, you construct at an accelerated pace. First, you put down a green half metre tall slab, then a quarter metre tall slab that's orange, then a green eight-metre slab, then an orange 16th meter slab and so on, until there are infinitely many layers of alternating color. Now, when you look at the cube from above, what color will you see? Orange, green? Well, it can't be orange because every orange layer is covered by a green one. And it can't be green because every green layer is blocked by an orange one above it.



What if we had a machine display each digit of Pi in order at a supertask pace? After its finite runtime, what would be on the screen? The last digit of Pi? Well, that's impossible, right? But how could it be anything else?



A supertask allows us to exhaust an infinite sequence. Paul Benacerraf delivered what is often considered the best response to these confusions. Is Thomson's lamp on or off? Is the cube orange or green? The answer is we don't know, because these questions are incomplete. Thomson's lamp could be on or off or broken. The cube could appear orange or green or something else. But the supertasks, as stated, don't let us figure out which. I may as well ask you if a lamp hidden in a locked room is on or off. It's definitely one or the other, but I haven't given you enough information to do anything but guess. Supertasks like these describe an endless sequence of tasks and then ask us about the end. But we can't determine an outcome because, although there may be an end to their duration, there is no end, no final member of their actions. They must be reworded or coupled with extra assumptions in order to be solved. For example, if we assume that the switch used on Thomson's lamp can only be all the way on or all the way off, we can't determine where it is after the supertask. But if the switch is, say, a bouncing ball that completes the circuit turning the lamp on each time it bounces on a metal plate, an outcome can be determined. If the physics here are ideal and the ball bounces half as high and half the time as it did on the previous balance, its sequence of bounce heights will turn the lamp on and off an infinite number of times in a finite amount of time. Although this bouncing ball has no penultimate state, no second-to-last bounce, it does have an ultimate state, a final one, resting on the plate. The circuit will be complete and the lamp will be on. You can also describe a switch on Thomson's lamp that leaves it off.



Sometimes, the next state after infinitely many isn't paradoxical because of lack of information, but because of a surprising, or non-intuitive, discontinuity that occurs there. The Ross–Littlewood paradox is one of the greatest examples. Imagine a giant urn that can hold an unlimited number of balls. Now, imagine that you have an unlimited supply of balls, each with a unique natural number written on it. All natural numbers, in fact, since there's no end to how many balls you have. Now, working at an accelerated Zenoian pace you move the balls to the urn 10 at a time, but in a weird way. At step one, you place balls number 1 to 10 in the urn, but remove number 1. At step two, a minute later, you place balls 11 to 20 in the urn and remove ball number 2. At step three, you place balls 21 to 30 in the urn and remove ball 3, and so on. Upon the completion of the supertask how many balls will be in the urn? At first the answer seems obvious. At each step you are adding 10 balls and subtracting 1, so a net of nine balls is added each time. 9 + 9 + 9 + 9 forever, the series grows without end. Infinite nine's means infinite balls at the end. But here's the problem.



At each step, the ball with that step's number written on it is removed. Ball 1 is removed at step one, ball 2 was removed at step two, ball 12-googol is removed at step twelve-googol. Since there are an endless number of steps for any ball number, there is a step number at which it is removed. So although the urn's ball population grows without bound during the task, after the supertask the number drops to 0. It gets weirder.



Here's a second, seemingly identical method. Instead of beginning with balls 1 to 10 and then removing 1, begin with balls 1 to 9. Then write zero after the "1" on ball 1. For step two, add balls 11 to 19 and draw a zero on ball 2, making it say 20. For every finite step, both methods result in identical earned contents, but after infinitely many steps, the first leaves us with no balls and the second leaves us with infinitely many balls written on which are all the natural numbers, each followed by an infinite string of zeros. Both are discontinuous at infinity, but dang, in very different ways.



The bigger question now becomes, "so what? Who cares?" You will never have an infinite number of balls and you will never have a large enough to urn to hold all of them. You will never build a lamp that can turn on and off arbitrarily fast. We cannot investigate time or space past a certain smallness, except when pretending, so what are supertasks, but recreational fictions, entertaining riddles? We can ask more questions than we can answer, so what?



Well, here's what. Neanderthals. Neanderthals and humans, us, Homo sapiens, lived together in Europe for at least five thousand years. Neanderthals were strong and clever, they may have even intentionally buried their dead, but for hundreds of thousands of years, Neanderthals barely went anywhere. They pretty much just explored and spread until they reached water or some other obstacle and then stopped. Homo sapiens, on the other hand, didn't do that. They did things that make no sense crossing terrain and water without knowing what lay ahead. Svante Pääbo has worked on the Neanderthal genome at the Max Planck Institute for Evolutionary Anthropology and he points out that technology alone didn't allow humans to go to Madagascar, to Australia. Neanderthals built boats too. Instead, he says, there's "some madness there. How many people must have sailed out and vanished on the Pacific before you found Easter Island? I mean, it's ridiculous. And why do you do that? Is it for the glory? For immortality? For curiosity? And now we go to Mars. We never stop." It's ridiculous, foolish, maybe? But it was the Neanderthals who went extinct, not the humans.



Maybe it's only a fool who will perilously journey out to what might not be there. And maybe it's only a fool who will ask about supertasks, about infinity. But if you want to solve problems, you don't just solve the ones that are there, you find more and make more and go after the impossible ones; fostering a love and obsession with problems is how you solve problems. Antoine de Saint-Exupéry wasn't a mathematician, but his advice fits nicely here. If you want to build a ship, don't drum up people to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea. And as always, thanks for watching.



Supertasks are cool, but super gifts are even cooler. That's why I'm excited to announce this year's Vsauce holiday box. This thing comes loaded with exclusive Vsauce stuff and science gear, plus all Vsauce proceeds go directly to Alzheimer's research. I'm really proud of this box. You can pick one up at geekfuel.com/Vsauce, link down in the description. There's a limited amount available, so don't hesitate. And as always, thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. There's a famous way to seemingly create chocolate out of nothing. Maybe you've seen it before. This chocolate bar is 4 squares by 8 squares, but if you cut it like this and then like this and finally like this you can rearrange the pieces like so and wind up with the same 4 by 8 bar but with a leftover piece, apparently created out of thin air. There's a popular animation of this illusion as well. I call it an illusion because it's just that. Fake. In reality, the final bar is a bit smaller. It contains this much less chocolate. Each square along the cut is shorter than it was in the original, but the cut makes it difficult to notice right away. The animation is extra misleading, because it tries to cover up its deception. The lost height of each square is surreptitiously added in while the piece moves to make it hard to notice. I mean, come on, obviously you cannot cut up a chocolate bar and rearrange the pieces into more than you started with. Or can you? 



One of the strangest theorems in modern mathematics is the Banach-Tarski paradox. It proves that there is, in fact, a way to take an object and separate it into 5 different pieces. And then, with those five pieces, simply rearrange them. No stretching required into two exact copies of the original item. Same density, same size, same everything. Seriously. To dive into the mind blow that it is and the way it fundamentally questions math and ourselves, we have to start by asking a few questions. 



First, what is infinity? A number? I mean, it's nowhere on the number line, but we often say things like there's an infinite "number" of blah-blah-blah. And as far as we know, infinity could be real. The universe may be infinite in size and flat, extending out for ever and ever without end, beyond even the part we can observe or ever hope to observe. That's exactly what infinity is. Not a number per se, but rather a size. The size of something that doesn't end. Infinity is not the biggest number, instead, it is how many numbers there are. But there are different sizes of infinity. The smallest type of infinity is countable infinity. The number of hours in forever. It's also the number of whole numbers that there are, natural number, the numbers we use when counting things, like 1, 2, 3, 4, 5, 6 and so on. Sets like these are unending, but they are countable. 



Countable means that you can count them from one element to any other in a finite amount of time, even if that finite amount of time is longer than you will live or the universe will exist for, it's still finite. 



Uncountable infinity, on the other hand, is literally bigger. Too big to even count. The number of real numbers that there are, not just whole numbers, but all numbers is uncountably infinite. You literally cannot count even from 0 to 1 in a finite amount of time by naming every real number in between. I mean, where do you even start? Zero, okay. But what comes next? 0.000000... Eventually, we would imagine a 1 going somewhere at the end, but there is no end. We could always add another 0. Uncountability makes this set so much larger than the set of all whole numbers that even between 0 and 1, there are more numbers than there are whole numbers on the entire endless number line. 



Georg Cantor's famous diagonal argument helps illustrate this. Imagine listing every number between zero and one. Since they are uncountable and can't be listed in order, let's imagine randomly generating them forever with no repeats. Each number regenerate can be paired with a whole number. If there's a one to one correspondence between the two, that is if we can match one whole number to each real number on our list, that would mean that countable and uncountable sets are the same size. But we can't do that, even though this list goes on forever. Forever isn't enough. Watch this. If we go diagonally down our endless list of real numbers and take the first decimal of the first number and the second of the second number, the third of the third and so on and add one to each, subtracting one if it happens to be a nine, we can generate a new real number that is obviously between 0 and 1, but since we've defined it to be different from every number on our endless list and at least one place it's clearly not contained in the list. In other words, we've used up every single whole number, the entire infinity of them and yet we can still come up with more real numbers. Here's something else that is true but counter-intuitive. There are the same number of even numbers as there are even and odd numbers. At first, that sounds ridiculous. Clearly, there are only half as many even numbers as all whole numbers, but that intuition is wrong. The set of all whole numbers is denser but every even number can be matched with a whole number. You will never run out of members either set, so this one to one correspondence shows that both sets are the same size. In other words, infinity divided by two is still infinity. Infinity plus one is also infinity. 



A good illustration of this is Hilbert's paradox up the Grand Hotel. Imagine a hotel with a countably infinite number of rooms. But now, imagine that there is a person booked into every single room. Seemingly, it's fully booked, right? No. Infinite sets go against common sense. You see, if a new guest shows up and wants a room, all the hotel has to do is move the guest in room number 1 to room number 2. And a guest in room 2 to room 3 and 3 to 4 and 4 to 5 and so on. Because the number of rooms is never ending we cannot run out of rooms. Infinity -1 is also infinity again. If one guest leaves the hotel, we can shift every guest the other way. Guest 2 goes to room 1, 3 to 2, 4 to 3 and so on, because we have an infinite amount of guests. That is a never ending supply of them. No room will be left empty. As it turns out, you can subtract any finite number from infinity and still be left with infinity. It doesn't care. It's unending. 



Banach-Tarski hasn't left our sights yet. All of this is related. We are now ready to move on to shapes. 



Hilbert's hotel can be applied to a circle. Points around the circumference can be thought of as guests. If we remove one point from the circle that point is gone, right? Infinity tells us it doesn't matter. The circumference of a circle is irrational. It's the radius times 2Pi. So, if we mark off points beginning from the whole, every radius length along the circumference going clockwise we will never land on the same point twice, ever. We can count off each point we mark with a whole number. So this set is never-ending, but countable, just like guests and rooms in Hilbert's hotel. And like those guests, even though one has checked out, we can just shift the rest. Move them counterclockwise and every room will be filled Point 1 moves to fill in the hole, point 2 fills in the place where point 1 used to be, 3 fills in 2 and so on. Since we have a unending supply of numbered points, no hole will be left unfilled. The missing point is forgotten. We apparently never needed it to be complete. 



There's one last needo consequence of infinity we should discuss before tackling Banach-Tarski. Ian Stewart famously proposed a brilliant dictionary. One that he called the Hyperwebster. The Hyperwebster lists every single possible word of any length formed from the 26 letters in the English alphabet. It begins with "a," followed by "aa," then "aaa," then "aaaa." And after an infinite number of those, "ab," then "aba," then "abaa", "abaaa," and so on until "z, "za," "zaa," et cetera, et cetera, until the final entry in infinite sequence of "z"s. Such a dictionary would contain every single word. Every single thought, definition, description, truth, lie, name, story. What happened to Amelia Earhart would be in that dictionary, as well as every single thing that didn't happened to Amelia Earhart. Everything that could be said using our alphabet. Obviously, it would be huge, but the company publishing it might realize that they could take a shortcut. If they put all the words that begin with a in a volume titled "A," they wouldn't have to print the initial "a." Readers would know to just add the "a," because it's the "a" volume. By removing the initial "a," the publisher is left with every "a" word sans the first "a," which has surprisingly become every possible word. Just one of the 26 volumes has been decomposed into the entire thing. It is now that we're ready to investigate this video's titular paradox. What if we turned an object, a 3D thing into a Hyperwebster? Could we decompose pieces of it into the whole thing? Yes. 



The first thing we need to do is give every single point on the surface of the sphere one name and one name only. A good way to do this is to name them after how they can be reached by a given starting point. If we move this starting point across the surface of the sphere in steps that are just the right length, no matter how many times or in what direction we rotate, so long as we never backtrack, it will never wind up in the same place twice. We only need to rotate in four directions to achieve this paradox. Up, down, left and right around two perpendicular axes. We are going to need every single possible sequence that can be made of any finite length out of just these four rotations. That means we will need lef, right, up and down as well as left left, left up, left down, but of course not left right, because, well, that's backtracking. Going left and then right means you're the same as you were before you did anything, so no left rights, no right lefts and no up downs and no down ups. Also notice that I'm writing the rotations in order right to left, so the final rotation is the leftmost letter. That will be important later on. Anyway. 



A list of all possible sequences of allowed rotations that are finite in lenght is, well, huge. Countably infinite, in fact. But if we apply each one of them to a starting point in green here and then name the point we land on after the sequence that brought us there, we can name a countably infinite set of points on the surface. Let's look at how, say, these four strings on our list would work. Right up left. Okay, rotating the starting point this way takes us here. Let's colour code the point based on the final rotation in its string, in this case it's left and for that we will use purple. Next up down down. That sequence takes us here. We name the point DD and color it blue, since we ended with a down rotation. RDR, that will be this point's name, takes us here. And for a final right rotation, let's use red. Finally, for a sequence that end with up, let's colour code the point orange. Now, if we imagine completing this process for every single sequence, we will have a countably infinite number of points named and color-coded. That's great, but not enough. 



There are an uncountably infinite number of points on a sphere's surface. But no worries, we can just pick a point we missed. Any point and color it green, making it a new starting point and then run every sequence from here. After doing this to an uncountably infinite number of starting point we will have indeed named and colored every single point on the surface just once. With the exception of poles. Every sequence has two poles of rotation. Locations on the sphere that come back to exactly where they started. For any sequence of right or left rotations, the polls are the north and south poles. The problem with poles like these is that more than one sequence can lead us to them. They can be named more than once and be colored in more than one color. For example, if you follow some other sequence to the north or south pole, any subsequent rights or lefts will be equally valid names. In order to deal with this we're going to just count them out of the normal scheme and color them all yellow. Every sequence has two, so there are a countably infinite amount of them. Now, with every point on the sphere given just one name and just one of six colors, we are ready to take the entire sphere apart. Every point on the surface corresponds to a unique line of points below it all the way to the center point. And we will be dragging every point's line along with it. The lone center point we will set aside. 



Okay, first we cut out and extract all the yellow poles, the green starting points, the orange up points, the blue down points and the red and purple left and right points. That's the entire sphere. With just these pieces you could build the whole thing. But take a look at the left piece. It is defined by being a piece composed of every point, accessed via a sequence ending with a left rotation. If we rotate this piece right, that's the same as adding an "R" to every point's name. But left and then right is a backtrack, they cancel each other out. And look what happens when you reduce them away. The set becomes the same as a set of all points with names that end with L, but also U, D and every point reached with no rotation. That's the full set of starting points. We have turned less than a quarter of the sphere into nearly three-quarters just by rotating it. We added nothing. It's like the Hyperwebster. If we had the right piece and the poles of rotation and the center point, well, we've got the entire sphere again, but with stuff left over. 



To make a second copy, let's rotate the up piece down. The down ups cancel because, well, it's the same as going nowhere and we're left with a set of all starting points, the entire up piece, the right piece and the left piece, but there's a problem here. We don't need this extra set of starting points. We still haven't used the original ones. No worries, let's just start over. We can just move everything from the up piece that turns into a starting point when rotated down. That means every point whose final rotation is up. Let's put them in the piece. Of course, after rotating points named UU will just turn into points named U, and that would give us a copy here and here. So, as it turns out, we need to move all points with any name that is just a string of Us. We will put them in the down piece and rotate the up piece down, which makes it congruent to the up right and left pieces, add in the down piece along with some up and the starting point piece and, well, we're almost done. The poles of rotation and center are missing from this copy, but no worries. There's a countably infinite number of holes, where the poles of rotations used to be, which means there is some pole around which we can rotate this sphere such that every pole hole orbits around without hitting another. Well, this is just a bunch of circles with one point missing. We fill them each like we did earlier. And we do the same for the centerpoint. Imagine a circle that contains it inside the sphere and just fill in from infinity and look what we've done. 



We have taken one sphere and turned it into two identical spheres without adding anything. One plus one equals 1. That took a while to go through, but the implications are huge. And mathematicians, scientists and philosophers are still debating them. Could such a process happen in the real world? I mean, it can happen mathematically and math allows us to abstractly predict and describe a lot of things in the real world with amazing accuracy, but does the Banach-Tarski paradox take it too far? Is it a place where math and physics separate? 



We still don't know. History is full of examples of mathematical concepts developed in the abstract that we did not think would ever apply to the real world for years, decades, centuries, until eventually science caught up and realized they were totally applicable and useful. The Banach-Tarski paradox could actually happen in our real-world, the only catch of course is that the five pieces you cut your object into aren't simple shapes. They must be infinitely complex and detailed. That's not possible to do in the real world, where measurements can only get so small and there's only a finite amount of time to do anything, but math says it's theoretically valid and some scientists think it may be physically valid too. There have been a number of papers published suggesting a link between by Banach-Tarski and the way tiny tiny sub-atomic particles can collide at high energies and turn into more particles than we began with. We are finite creatures. Our lives are small and can only scientifically consider a small part of reality. What's common for us is just a sliver of what's available. We can only see so much of the electromagnetic spectrum. We can only delve so deep into extensions of space. Common sense applies to that which we can access. 



But common sense is just that. Common. If total sense is what we want, we should be prepared to accept that we shouldn't call infinity weird or strange. The results we've arrived at by accepting it are valid, true within the system we use to understand, measure, predict and order the universe. Perhaps the system still needs perfecting, but at the end of day, history continues to show us that the universe isn't strange. We are. And as always, thanks for watching. Finally, as always, the description is full of links to learn more. There are also a number of books linked down there that really helped me wrap my mind kinda around Banach-Tarski. 



First of all, Leonard Wapner's "The Pea and the Sun." This book is fantastic and it's full of lot of the preliminaries needed to understand the proof that comes later. He also talks a lot about the ramifications of what Banach-Tarski and their theorem might mean for mathematics. 



Also, if you wanna talk about math and whether it's discovered or invented, whether it really truly will map onto the universe, Yanofsky's "The Outer Limits of Reason" is great. This is the favorite book of mine that I've read this entire year. Another good one is E. Brian Davies' "Why Beliefs Matter." This is actually Corn's favorite book, as you might be able to see there. It's delicious and full of lots of great information about the limits of what we can know and what science is and what mathematics is. 



If you love infinity and math, I cannot more highly recommend Matt Parker's "Things to Make and Do in the Fourth Dimension." He's hilarious and this book is very very great at explaining some pretty awesome things. So keep reading, and if you're looking for something to watch, I hope you've already watched Kevin Lieber's film on Field Day. I already did a documentary about Whittier, Alaska over there. Kevin's got a great short film about putting things out on the Internet and having people react to them. There's a rumor that Jake Roper might be doing something on Field Day soon. So check out mine, check out Kevin's and subscribe to Field Day for upcoming Jake Roper action, yeah? He's actually in this room right now, say hi, Jake. [Jake:] Hi. Thanks for filming this, by the way. Guys, I really appreciate who you all are. And as always, thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. This is called the dolly zoom effect and the optics that make it possible are also responsible for what's called the moon terminator illusion. A terminator is the line between an illuminated and dark side. Light arrives perpendicular to the terminator but it doesn't always appear to. Maybe you've noticed this before. When the Moon and the Sun are up at the same time, and a good distance apart in the sky, the direction the Moon is being lit from won't always appear to line up with the position of the Sun. What's going on here? The answer lies in visual angles. The further away from you something is, the smaller it will appear to be. In psychophysical terms, its visual angle shrinks - the angle within your field of view that it stretches across. Now, this means that everything we look at is foreshortened, that is it diminishes incise towards the horizon line. A line infinitely far away. Things that far away are foreshortened to the point at which they have no height whatsoever. This distortion affects everything that we look at. But it doesn't confuse us, because our brains know about it.



We can look at an object that is foreshortened and figure out what it probably actually looks like. For example, a door is a rectangle, but it's wrecked by foreshortening when opened. Closer distances appear larger. The whole shape is different, but your brain isn't freaked out every time a door opens, because it understands that different sensations don't necessarily mean different actual things. It's called subjective constancy. Our brains cleverly factor in previous experience and perspective clues from the surrounding world to calculate in real time whether changes in what we sense are because of actual changes in the things we're looking at or are just merely products of their positions or the surroundings. For example, I, right now on this video screen, am vulnerable to foreshortening. But I probably look okay to you. However, if we frame in really closely and remove surrounding perspective clues, I can be made to look weird, strange, a little scary, distended. I'm foreshortened without the necessary tools around me for you to mentally adjust. This is probably the crux of the Moon terminator illusion. Take a look at this.



The line where that wall meets the ceiling is a straight line. Seriously, take a look. If you look to the left and right, it continues to look straight and trust me, this building was built with level straight ceilings. But that's not what your eye senses. A camera will help demonstrate this. It doesn't add in the post sensation processing that our brains do and we can also frame in to remove other clues telling us what to think. Now, sure enough, right in front of us the line is level with the screen of the camera - it's a straight line - but if we move our eyes across the line, look what happens. If I pan this way, the slope changes and now the line appears to be going up and to the left. And if I panned this way, the line appears to be moving up and to the right. What's going on here?



A straight line can't have changing slope, that's what a curved line does. Just like the curve we think we see between the Sun and the Moon. But wait. If the line between the Sun and the Moon is curved because of foreshortening, why don't we see lines like this one as curves? Well, here's a clue. Clues. We know what rooms are supposed to look like. We know how they're built. We have experience with them and there are other things in the environment helping us. But when you look at the sky, there's not much to help you. In the absence of clues about distance and perspective, our brains assume that whatever we're looking at is equidistant from us, making the sky a sort of dome surface, like a planetarium screen. The only clue we have then is the horizon, but it's a red herring, because the horizon doesn't foreshorten. It's just a place where due to foreshortening everything becomes infinitely flat.



Brian Rogers and Olga Naumenko demonstrated that these things result in the terminator illusion by using a planetarium. They projected two dots onto its dome, mimicking our perception of the Sun and the Moon. They've then asked participants to place a third dot on the straight line connecting them. But just as in the Moon terminator illusion, people incorrectly placed the third dot. They were influenced by a desire to stay parallel to the horizon, as if it was subject to the same foreshortening rules. Our brains also often fail to factor in foreshortening when it comes to crepuscular rays, light beams streaming through gaps in clouds. They appear to converge from a common point, as if the Sun is only a few thousand meters above Earth, but, of course, in fact, the Sun is extremely far away and these lines are actually pretty much exactly parallel. It's true. They look like they converge for the same reason railroad tracks appear to converge. The visual angle of the distance separating them shrinks the further down you look. Sometimes our brains do the opposite. They assume and consider foreshortening even when it isn't really there, like in this illusion, where the cars seem to be different sizes but in actuality on the page are all the same size.



Everything else in the image is affected by foreshortening, so our brains assume that the cars are too and decide that in order for them all to have the same visual angle, which they do, the more distant ones must be literally larger in real life. The rate at which the visual angle of something you are looking at changes as you move toward it, or away from it, is not constant. In order to cut the visual angle of an object in half, its distance from you must double. That means objects really really far away require a lot more movement to have their visual angle changed compared to nearer objects. This is called parallax and it's a major reason moving allows us to learn so much about depth. It's why stuff on the side of the road whizzes is by you as you drive past, while distant features of the landscape appear to only crawl past. A really distant thing, like, say, the Moon, 384,000 kilometers away appears to move so little as you do, it can seem to be following you. It's not, of course. It's just geometry.



For the same reasons, moving a camera through space will affect the visual angle of nearer things, like the Vsauce mug here much more dramatically than distant things, like Jake Chudnow's album against the wall. It is barely changing size as we move back and forth. But unlike moving, zooming increases the visual angle of everything equally, regardless of depth. Notice that when zooming, the distant album grows and shrinks right along with the mug. Now, if you move IN while zooming OUT, or vice versa, the zoom will be most apparent when looking at distant things, while the move more apparent on near things. The result can be trippy. Here I've been placed where both effects cancel each other out, so everything around me changes instead. Another great illusion that takes advantage of parallax is the popular star field or hallway illusion. Cover the middle of the video and you'll appear to be moving much faster. But you if cover the edges, you'll appear to be moving much slower. All you're doing is altering the perspective clues your brain receives about movement. The nearer stuff at the edge changes position faster than the distance stuff way ahead. So if you cover the slow distance stuff your brain only has the faster-moving stuff to judge speed from. But cover the faster near stuff and the slower stuff becomes the new near. Your brain assumes that you've slowed down. The optics and mathematics behind all of these effects were understood at least thousands of years ago.



During the Renaissance, artists used that knowledge to produce paintings that imitated reality really well. I mean, the perspective is quite realistic. But... the renaissance? Why did it take so long for us get there? I mean, look at this pre-Renaissance painting. What the heck is going on here? The perspective is all wrong. That's not at all what such a scene would actually look like. Artists and viewers contemporary to works like that saw the world just like we do, they saw foreshortening - they couldn't help but see it. Foreshortening is right there in plain sight. No, foreshortening is plain sight. So, what gives? Were medieval artists all just a bunch of five-year-olds? No.



No in the sense that the question is wrong. Realistic-looking perspective was used way before the Renaissance. Now, it might not have always been mathematically formal perspective, but foreshortening was understood. Western art went from this to this, not because humans all of a sudden became smarter, but because of a difference in desire. What to us today might look unrealistic and thus bad, was, in its time, deliberate and popular. The shift to imitating the world mathematically, as if seen through a window, was more about cultural interests and objectivity and the individual than it was about artists all of a sudden becoming smarter. Furthermore, perspective illusions shows that what we think we see isn't always what we're seeing. A child's drawing may seem crude in terms of objective mathematical imitation, but there are other things about the world and our experience of it worth imitating. Teasing out the less plainly obvious to show something personal is sometimes even harder than following perspective grids Picasso put it this way it: "It took me four years to learn to paint like Rafael, but a lifetime to learn to paint like a child." And as always, thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. About 6 percent of everything you say and read and write is the "the" - is the most used word in the English language. About one out of every 16 words we encounter on a daily basis is "the." The top 20 most common English words in order are "the," "of," "and," "to," "a," "in," "is," "I," "that," "it," "for," "you," "was," "with," "on," "as," "have," "but," "be," "they." That's a fun fact. A piece of trivia but it's also more. You see, whether the most commonly used words are ranked across an entire language, or in just one book or article, almost every time a bizarre pattern emerges. The second most used word will appear about half as often as the most used. The third one third as often. The fourth one fourth as often. The fifth one fifth as often. The sixth one sixth as often, and so on all the way down. Seriously. 



For some reason, the amount of times a word is used is just proportional to one over its rank. Word frequency and ranking on a log log graph follow a nice straight line. A power-law. This phenomenon is called Zipf's Law and it doesn't only apply to English. It also applies to other languages, like, well, all of them. Even ancient languages we haven't been able to translate yet. And here's the thing. We have no idea why. It's surprising that something as complex as reality should be conveyed by something as creative as language in such a predictable way. How predictable? 



Well, watch this. According to WordCount.org, which ranks words as found in the British National Corpus, "sauce" is the 5,555th most common English word. Now, here is a list of how many times every word on Wikipedia and in the entire Gutenberg Corpus of tens of thousands of public domain books shows up. The most used word, 'the,' shows up about 181 million times. Knowing these two things, we can estimate that the word "sauce" should appear about thirty thousand times on Wikipedia and Gutenberg combined. And it pretty much does. What gives? The world is chaotic. Things are distributed in myriad of ways, not just power laws. And language is personal, intentional, idiosyncratic. What about the world and ourselves could cause such complex activities and behaviors to follow such a basic rule? We literally don't know. 



More than a century of research has yet to close the case. Moreover, Zipf's law doesn't just mysteriously describe word use. It's also found in city populations, solar flare intensities, protein sequences and immune receptors, the amount of traffic websites get, earthquake magnitudes, the number of times academic papers are cited, last names, the firing patterns of neural networks, ingredients used in cookbooks, the number of phone calls people received, the diameter of Moon craters, the number of people that die in wars, the popularity of opening chess moves, even the rate at which we forget. There are plenty of theories about why language is 'zipf-y,' but no firm conclusions and this video doesn't contain a definite explanation either. Sorry, I know that's a bummer, since we appear to like knowing more than mystery. But that said, we also ask more than we answer. So let's dive into Zipf's ramifications, some related patterns, some possible explanations and the depth of the mystery itself. 



Zipf's law was popularized by George Zipf, a linguist at Harvard University. It is a discrete form of the continuous Pareto distribution from which we get the Pareto Principle. Because so many real-world processes behave this way, the Pareto Principle tells us that, as a rule of thumb, it's worth assuming that 20% of the causes are responsible for 80% of the outcome, like in language, where the most frequently used 18 percent of words account for over 80% of word occurrences. In 1896, Vilfredo Pareto showed that approximately 80% of the land in Italy was owned by just twenty percent of the population. It is said that he later noticed in his garden 20 percent of his pea pods contained eighty percent of the peas. He and other researchers looked at other datasets and found that this 80-20 imbalance comes up a lot in the world. The richest 20% of humans have 82.7% of the world's income. In the US, 20% of patients use eighty percent of health care resources. In 2002, Microsoft reported that 80% of the errors and crashes in Windows and Office are caused by 20% of the bugs detected. A common rule of thumb in the business world states that 20% of your customers are responsible for 80% of your profits and eighty percent of the complaints you receive will come from 20% of your customers. A book titled "The 80/20 Principle" even says that in a home or office, 20% of the carpet receives 80 percent of the wear. Oh, and as Woody Allen famously said, "eighty percent of success is just showing up." The Pareto Principle is everywhere, which is good. By focusing on just 20 percent of what's wrong, you can often expect to solve eighty percent of the problems. 



A variety of different unrelated factors cause this to be true from case to case, but if we can get to the bottom of what causes some of them, maybe we'll find that one or more of those mechanisms is responsible for Zipf's law in language. George Zipf himself thought languages' interesting rank frequency distribution was a consequence of the Principle of Least Effort. The tendency for life and things to follow the path of least resistance. Zipf believed it drove much of human behavior and hypothesized that as language developed in our species, speakers naturally preferred drawing from as few words as possible to get their thoughts out there. It was easier. But in order to understand what was being said, listeners preferred larger vocabularies that gave more specificity, so that they had to do less work. The compromise between listening and speaking, Zipf felt, led to the current state of language. A few words are used often and many many many words are used rarely. 



Recent papers have suggested that having a few short, often used, predictable words helps dissipate information load density on listeners, spacing out important vocab so that the information rate is more constant. This makes sense and much has been learned by applying the least effort principle to other behaviors, but later researchers argued that for language, the explanation was even more simple. Just a few years after Zipf's seminal paper, Benoit Mandelbrot showed that there may be nothing mysterious about Zipf's law at all, because even if you just randomly type on a keyboard you will produce words distributed according to Zipf's law. It's a pretty cool point and this is why it happens. 



There are exponentially more different long words than short words. For instance, the English alphabet can be used to make 26 one letter words, but 26 squared 2 letter words. Also, in random typing, whenever the space bar is pressed a word terminates. Since there's always a certain chance that the space bar will be pressed, longer stretches of time before it happens are exponentially less likely than shorter ones. The combination of these exponentials is pretty 'Zipf-y.' For example, if all 26 letters and the spacebar are equally likely to be typed, after a letter is typed and a word has begun, the probability that the next input will be a space, thus creating a one letter word, is just one in 27. And sure enough, if you randomly generate characters or hire a proverbial typing monkey, about one out of every 27 or 3.7 percent of the stuff between spaces, will be single letters. Two letter words appear when after beginning a word any character but the space bar is hit - a 26 in 27 chance and then the space bar. A three-letter word is the probability of a letter, another letter and then a space. If we divide by the number of unique words of each length there can be, we get the frequency of occurrence expected for any particular word given its length. For example, the letter V will make up about 0.142 percent of random typing. The word "Vsauce" 0.0000000993 percent. Longer words are less likely, but watch this. Let's spread these frequencies out according to the ranks they'd take up on a most often used list. There are 26 possible one letter words, so each of the top 26 ranked words are expected to occur about this often. The next 676 ranks will be taken up by two letter words that show up about this often. If we extend each frequency according to how many members it has, we get Zipf. 



Subsequent researchers have detailed how changing up the initial conditions can smooth the steps out. Our mysterious distribution has been created out of nothing but the inevitabilities of math. So maybe there is no mystery. Maybe words are just the result of humans randomly segmenting the observable world and the mental world into labels and Zipf's law describes what naturally happens when you do that. Case closed. and as always And as always, thanks for... wait a minute! Actual language is very different from random typing. Communication is deterministic to a certain extent. Utterances and topics arrive based on what was said before. And the vocabulary we have to work with certainly isn't the result of purely random naming. For example, the monkey typing model can't explain why even the names of the elements, the planets and the days of the week are used in language according to Zipf's law. Sets like these are constrained by the natural world and they're not the result of us randomly segmenting the world into labels. 



Furthermore, when given a list of novel words, words they've never heard or used before, like when prompted to write a story about alien creatures with strange names, people will naturally tend to use the name of one alien twice as often as another, three times as often as another... Zipf's law appears to be built into our brains. Perhaps there is something about the way thoughts and topics of discussion ebb and flow that contributes to Zipf's law. 



Another way 'Zipf-ian' distributions occur is via processes that change according to how they've previously operated. These are called preferential attachment processes. They occur when something - money, views, attention, variation, friends, jobs, anything really is given out according to how much is already possessed. To go back to the carpet example, if most people walk from the living room to the kitchen across a certain path, furniture will be placed elsewhere, making that path even more popular. The more views a video or image or post has, the more likely it is to get recommended automatically or make the news for having so many views, both of which give it more views. It's like a snowball rolling down a snowy hill. The more snow it accumulates, the bigger its surface area becomes for collecting more and the faster it grows. There doesn't have to be a deliberate choice driving a preferential attachment process. It can happen naturally. Try this. 



Take a bunch of paper clips and grab any two at random. Link them together and then throw them back in the pile. Now, repeat over and over again. If you grab paper clips that are already part of a chain, link 'em anyway. More often than not after a while you will have a distribution that looks Zipf-ian.' A small number of chains contain a disproportionate amount of the total paperclip count. This is simply because the longer a chain gets, the greater proportion of the whole it contains, which gives it a better chance of being picked up in the future and consequently made even longer. The rich get richer, the big get bigger, the popular get popular- er. It's just math. 



Perhaps languages' Zipf mystery is, if not caused by it, at least strengthened by preferential attachment. Once a word is used, it's more likely to be used again soon. Critical points may play a role as well. Writing and conversation often stick to a topic until a critical point is reached and the subject is changed and the vocabulary shifts. Processes like these are known to result in power laws. 



So, in the end, it seems tenable that all these mechanisms might collude to make Zipf's law the most natural way for language to be. Perhaps some of our vocabulary and grammar was developed randomly, according to Mandelbrot's theory. And the natural way conversation and discussion follow preferential attachment and criticality, coupled with the principle of least effort when speaking and listening are all responsible for the relationship between word rank and frequency. It's a shame that the answer isn't simpler, but it's fascinating because of the consequences it has on what communication is made of. Roughly speaking, and this is mind blowing, nearly half of any book, conversation or article will be nothing but the same 50 to 100 words. And nearly the other half will be words that appear in that selection only once. That's not so surprising when you consider the fact that one word accounts for 6 percent of what we say. 



The top 25 most used words make up about a third of everything we say and the top 100 about half. Seriously. I mean, whether it's all the words in "Wet Hot American Summer," or all the words in Plato's "Complete Works" or in the complete works of Edgar Allan Poe or the Bible itself, only about 100 words are used for nearly half of everything written or said. In Alice's Adventures in Wonderland 44% and in Tom Sawyer 49.8% of the unique words used appear only once in the book. 



A word that is used only once in a given selection of words is called a 'hapax legomenon.' Hapax legomena are vitally important to understanding languages. If a word has only been found once in the entire known collection of an ancient language, it can be very difficult to figure out what it means. Now, there is no corpus of everything ever said or written in English, but there are very very large collections and it's fun to find hapax legomena in them. For instance, and this probably won't be the case after I mention it, but the word "quizzaciously" is in the Oxford English Dictionary, but appears nowhere on Wikipedia or in the Gutenberg corpus or in the British National Corpus or the American National Corpus, but it does appear when searched in just one result on Google. Fittingly, in a book titled "ElderSpeak" that lists it as a 'rare word.' Quizzaciously, by the way, means "in a mocking manner," as in "The paradist rattled off quizzaciously, ‘Hey, Vsauce. Michael here. But who is Michael and how much does here weigh?'" It's a little sad that quizzaciously has been used so infrequently. It's a fun word, but that's the way things go in a 'Zipf-ian' system. 



Some things get all the love, some get little. Most of what you experience on a day-to-day basis is forgotten, forgettable. The Dictionary of Obscure Sorrows, as it often does, has a word for this - Olēka - the awareness of how few days are memorable. I've been alive for almost 11,000 days but I couldn't tell you something about each one of them. I mean, not even close. Most of what we do and see and think and say and hear and feel is forgotten at a rate quite similar to Zipf's law, which makes sense. If a number of factors naturally selected for thinking and talking about the world with tools in a 'Zipf-ian' way, it makes sense we'd remember it that way too. Some things really well, most things hardly at all. But it bums me out sometimes because it means that so much is forgotten, even things that at the time you thought you could never forget. My locker number - senior year - its combination, the jokes I liked when I saw a comedian on stage, the names of people I saw every day 10 years ago. So many memories are gone. When I look at all the books I've read and realize that I can't remember every detail from them, it's a little disappointing. I mean, why even bother if the Pareto Principle dictates that my 'Zipf-ian' mind will consciously remember pretty much only the titles and a few basic reactions years later Ralph Waldo Emerson makes me feel better. He once said, "I cannot remember the books I've read any more than the meals I have eaten. Even so, they have made me." 



And as always, thanks for watching.

<|endoftext|>

<|startoftext|>

Hey, Vsauce. Michael here. According to the U.S. Census Bureau right now, in America, there are 106 people named Harry Potter. 1 007 named James Bond and eight people named Justin Bieber. They're just aren't enough names to go around. There are more than 300 million people in America but a hundred and fifty thousand last names and five thousand first names is all you need to name 9 out of every 10 of them.



When are we gonna run out of names? Perhaps it's already happened to you. If it hasn't, when? Ten years, twenty years, a hundred, a thousand. When will someone with your exact name become famous? So famous in fact that your legacy changes forever to just being not the person people think of when they hear your name. And for that matter, when will every reasonably memorable pronounceable band name or brand name be taken? When will authors have no choice but to just start reusing book titles? According to Rovi Corp, owner of AllMusic.com, the most used band name is 'Bliss', followed in order by 'Mirage', 'One', Gemini', 'Legacy', 'Paradox' and 'Rain'. In the past when fewer bands had already been created and you couldn't just Google up every single band, overlap was easier to get away with and one word band names were plentiful. But now, after years and years and years of band formation, well, we have The Who', but we also have 'The What?', The Where', 'The When', 'The Why', 'The How' and even 'The The'.



In order to stand out now, and have your own unique name, you have to be a bit more creative. O', 'Diarrhea Planet' or 'Betty's Not a Vitamin', which, by the way, is no longer true. Betty became a vitamin in 1994. What about Twitter handles or email addresses? Have we already reached peak username? We already find ourselves often having to use abbreviations, initials, numbers or just choosing something completely different. Will our children or our children's children live in a world where the only remaining Gmail addresses is are just random strings of alphanumeric characters? Are we approaching a name crisis? And if so, should we even call it a name crisis, lest we use up yet another precious name?



Maybe you already share your name with someone famous. But if you don't, how long will it be until you probably will?



I mean, new famous people are popping up all the time faster now than ever before because of the Internet and they are gobbling up top Google search billing. Maybe it won't happen until long after you've been dead but shouldn't the reservoir names, not taken by notable people, eventually run out? Computer scientist Samuel Arbesman approximated how many famous people there are alive today and I think his calculation will be helpful. You see, he points out that if we allow "famous" to simply mean "being notable enough to have your own Wikipedia page", well, because there are 700,000 living people with Wikipedia pages right now, that means one out of every 10,000 people on earth today are famous. Assuming at the least that that proportion remains constant since 255 people are born every minute, that means every hour a future famous person is born. Their name destined to become primarily associated with them, not everyone else who shares their name. All of those people will be relegated to disambiguation or the post-nominal, not the famous one.



Luckily, if you do the math, you'll find that even at a rate of one future famous person born every hour, it would still take dozens of millennia for most of us to expect a future famous person with our exact name to emerge. Plus names change. New ones become popular, others obsolesce, but for fun, let's not focus on names we popularly use and instead look at how many possible names there can be.



The Social Security Administration allows up to 36 letters for a complete name. Now, including spaces, 27 letters filling 36 spots, with repetition allowed, means 3 sextillion possible combinations. That's more than Earth has atoms. So let's refine our limits. How many pronounceable names are possible? For that, I say we look at what Randall Munroe, the author of the fantastic 'What If?' did when asked about naming stars. If you want to give every single star in the observable universe a unique but pronounceable English name, how long would the names have to be? His approximation is really fascinating. If we define a pronounceable word as a word that contains consonant-vowel pairs, we can roughly figure that there are about 105 different such pair possibilities. 105. That's not too much different from 99. So, funny enough, there are about the same number of consonant-vowel pairs possible as there are two-digit numbers, which means we could give every star in the observable universe a sayable unique name with just 24 letters, the same number of digits it would take to just number all of them.



So, the bottom line is we may each have to give up uniquely owning a word or name that's common today, but the potential number of names that can be made is really hardy. In fact, before we run out of those our species will likely evolve to communicate in a completely different way. Also, names aren't just labels. A name on a screen, a username, a handle, a screen name doesn't always act exactly like its owner. User names can travel more quickly and more widely than flesh- and-blood people and do things that their puppeteers wouldn't normally do away from the keyboard. It's called the online disinhibition effect. If you can't see the people you're interacting with and they can't see you, you're all just online hiding behind different names than usual, why hold back? I mean, clearly such a system can't be serious business. On the Internet no one knows you're a dog. Why be nice or tell the truth?



The subreddit KarmaCourt investigates and uncovers people who may think that the less face-to-face nature of the Internet makes lies easier to get away with. Like this person, who posted an image suggesting that they've been single for a year but had users found out posted three months earlier a picture of my girlfriend's cat. These behaviors aren't just what humans do when they can be anonymous or can hide behind different names, these behaviors can also be caused by the names themselves. Studies have found that the username you use can impact how you behave. Your own pre-existing stereotypes and expectations of certain words, shapes, colors can be confirmed by your behavior, in the same way that studies have found NFL and NHL players play more aggressively when wearing black uniforms. Studies have also found that the more sexualized an avatar is you make someone use, the more conscious they will be of their own body image. And the more an avatar resembles you, the more correlated you watching it exercise is with you being more likely to exercise more. It's called the Proteus effect.



